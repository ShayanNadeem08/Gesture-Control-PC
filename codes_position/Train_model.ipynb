{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf4d36-4338-4911-a08e-c5f08df798e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IN THE NAME OF  ALLAH , THE MOST GRACIOUS, THE MOST MERCIFUL. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25326091-165a-4f57-8abf-7070d8bb0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "modules_path = '../external_modules/'\n",
    "dataset_path = '../../Dataset/STMM'\n",
    "save_path = \"../model\"\n",
    "\n",
    "sys.path.insert(1, modules_path)\n",
    "from video_dataset import VideoFrameDataset, ImglistToTensor\n",
    "\n",
    "if False and torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f832a2f-7ddf-471d-b04b-3b7233438c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "#img_w = 640\n",
    "#img_h = 480\n",
    "#frames_per_video = 7\n",
    "batch_size = 8\n",
    "num_workers = 1\n",
    "\n",
    "dataset = VideoFrameDataset(\n",
    "    root_path= f\"{dataset_path}\",\n",
    "    annotationfile_path=f\"{dataset_path}/annotations.txt\",\n",
    "    num_segments=8,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='{:01d}.jpg',\n",
    "    transform=ImglistToTensor(),\n",
    "    test_mode=False\n",
    ")\n",
    "class_map = {0:\"down\", 1:\"left\", 2:\"right\", 3:\"up\"}\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [0.7,0.2,0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f642bad4-d373-4a2f-945b-4bd9ce5b6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessings\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def get_hand_landmarks_frame(frame):\n",
    "    frame = 255*frame\n",
    "    frame = frame.transpose(2,0).numpy().astype('uint8')\n",
    "    results = hands.process(frame)\n",
    "\n",
    "    hand_landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            hand_landmarks.append([hand_landmark.x, hand_landmark.y, hand_landmark.z])\n",
    "        hand_landmarks = np.array(hand_landmarks).flatten()\n",
    "    else:\n",
    "        hand_landmarks = np.zeros(63)\n",
    "\n",
    "    return torch.from_numpy(hand_landmarks.astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33486d79-c9b6-4082-af45-4e3fd03bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NN_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NN_LSTM, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(layer_sizes[0],layer_sizes[1])\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.lstm = torch.nn.LSTM(layer_sizes[1],layer_sizes[2],batch_first=True)\n",
    "        self.linear2 = torch.nn.Linear(layer_sizes[2],layer_sizes[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.linear1(x)\n",
    "        out2 = self.relu1(out1)\n",
    "        out3 = self.lstm(out2)\n",
    "        out4 = self.linear2(out3[0])\n",
    "        return out4\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "model = NN_LSTM([63,10,5,4]).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2459db1b-6d08-43d1-b6f3-241ff56cd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train(model, data):\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        print(\"\\r\"+str(i), end=\"\")\n",
    "            \n",
    "        y = y.to(device)\n",
    "        hand_landmarks = torch.zeros(batch_size, 8, 63)\n",
    "\n",
    "        for i, video in enumerate(x):    # x is batch\n",
    "            for j, frame in enumerate(video):\n",
    "                hand_landmarks[i][j] = get_hand_landmarks_frame(frame)\n",
    "        \n",
    "        y_hat = model(hand_landmarks)\n",
    "        y_hat = y_hat.transpose(0,1)[-1]\n",
    "        #y_hat = y_hat.transpose(0,1)[0]\n",
    "        loss = loss_function(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, data, label=\"\"):\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y) in enumerate(data):\n",
    "            hand_landmarks = torch.zeros(batch_size, 8, 63)\n",
    "\n",
    "            for i, video in enumerate(x):    # x is batch\n",
    "                for j, frame in enumerate(video):\n",
    "                    hand_landmarks[i][j] = get_hand_landmarks_frame(frame)\n",
    "            \n",
    "            y_hat = model(hand_landmarks)\n",
    "            y_hat = y_hat.transpose(0,1)[-1]\n",
    "            _, y_hat = torch.max(y_hat,1)\n",
    "            correct += (y_hat==y).sum()\n",
    "    print(label+\"accuracy:\", round(float(correct/(i+1)/batch_size), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c9c0ded-4d9a-4614-bbb0-81950d59b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160  Epoch: 0  Loss: 1.413\n",
      "160  Epoch: 1  Loss: 0.865\n",
      "160  Epoch: 2  Loss: 1.234\n",
      "160  Epoch: 3  Loss: 1.097\n",
      "160  Epoch: 4  Loss: 1.003\n",
      "160  Epoch: 5  Loss: 1.146\n",
      "160  Epoch: 6  Loss: 0.862\n",
      "160  Epoch: 7  Loss: 0.884\n",
      "160  Epoch: 8  Loss: 1.013\n",
      "88"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000017D12899B20>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1568, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\PMLS\\anaconda3\\Lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PMLS\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py\", line 112, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, video \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x):    \u001b[38;5;66;03m# x is batch\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(video):\n\u001b[1;32m---> 12\u001b[0m         hand_landmarks[i][j] \u001b[38;5;241m=\u001b[39m get_hand_landmarks_frame(frame)\n\u001b[0;32m     14\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model(hand_landmarks)\n\u001b[0;32m     15\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mget_hand_landmarks_frame\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     14\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\u001b[38;5;241m*\u001b[39mframe\n\u001b[0;32m     15\u001b[0m frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(frame)\n\u001b[0;32m     18\u001b[0m hand_landmarks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train\n",
    "NumEpochs=12\n",
    "for epoch in range(NumEpochs):\n",
    "    loss = train(model, train_loader)\n",
    "    print(\"  Epoch:\",epoch, \" Loss:\", round(loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eadea647-0243-409d-bab1-807155fa4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, save_path+\"/model_lstm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bdb60-7ed6-4146-81d7-338bd02f9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(save_path+\"/model4_dropout.model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7aac234-dcf3-4c76-8e94-7057d865f3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingaccuracy: 10.562\n",
      "Vailidationaccuracy: 3.25\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test(model, train_loader, \"Training\")\n",
    "test(model, valid_loader, \"Vailidation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0563009-2392-4ddb-a487-63def436e829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

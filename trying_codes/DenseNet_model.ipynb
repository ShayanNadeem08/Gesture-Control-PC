{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "17a5ae72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "17a5ae72",
        "outputId": "67bf57c5-0e5a-4705-b730-995454d5291f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: '../Dataset/STMM/image\\\\down'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\PMLS\\FYP\\Gesture-Control-PC\\codes\\DenseNet_model.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=456'>457</a>\u001b[0m data_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../Dataset/STMM/image\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=458'>459</a>\u001b[0m \u001b[39m# Load and preprocess the dataset\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=459'>460</a>\u001b[0m X, y, subject_ids \u001b[39m=\u001b[39m load_dataset(data_path)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=461'>462</a>\u001b[0m \u001b[39m# Print dataset info\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=462'>463</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset shape: \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\PMLS\\FYP\\Gesture-Control-PC\\codes\\DenseNet_model.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m gesture_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, gesture)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Iterate through sequence folders for this gesture\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq_folder \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(gesture_path):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     seq_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(gesture_path, seq_folder)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(seq_path):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         \u001b[39m# Extract subject ID from sequence folder name (assuming format contains subject ID)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/DenseNet_model.ipynb#W0sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39m# Modify this according to your actual folder naming convention\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Dataset/STMM/image\\\\down'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "# Custom Dataset class for hand gestures\n",
        "class HandGestureDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# Function to load and preprocess the dataset\n",
        "def load_dataset(data_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the hand gesture dataset.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to the dataset directory\n",
        "\n",
        "    Returns:\n",
        "        X: Sequences of images with shape (num_sequences, channels, frames, height, width)\n",
        "        y: Labels corresponding to gestures (0: down, 1: left, 2: right, 3: up)\n",
        "        subject_ids: IDs of subjects for leave-one-subject-out cross-validation\n",
        "    \"\"\"\n",
        "    # Placeholder arrays for data, labels, and subject IDs\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    subject_ids = []\n",
        "\n",
        "    # Map gesture names to numerical labels\n",
        "    gesture_map = {'down': 0, 'left': 1, 'right': 2, 'up': 3}\n",
        "\n",
        "    # Image transformation pipeline\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),  # Using power of 2 dimensions for better downsampling\n",
        "        transforms.ToTensor(),  # Converts to [0,1] range and changes to CxHxW format\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] range\n",
        "    ])\n",
        "\n",
        "    # Iterate through gesture folders\n",
        "    for gesture in ['down', 'left', 'right', 'up']:\n",
        "        gesture_path = os.path.join(data_path, gesture)\n",
        "\n",
        "        # Iterate through sequence folders for this gesture\n",
        "        for seq_folder in os.listdir(gesture_path):\n",
        "            seq_path = os.path.join(gesture_path, seq_folder)\n",
        "\n",
        "            if os.path.isdir(seq_path):\n",
        "                # Extract subject ID from sequence folder name (assuming format contains subject ID)\n",
        "                # Modify this according to your actual folder naming convention\n",
        "                subject_id = int(seq_folder.split('_')[0])\n",
        "\n",
        "                # Load frames for this sequence\n",
        "                frame_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.jpg') or f.endswith('.png')])\n",
        "\n",
        "                if len(frame_files) > 0:\n",
        "                    # Load and normalize frames\n",
        "                    frames = []\n",
        "                    for frame_file in frame_files:\n",
        "                        frame_path = os.path.join(seq_path, frame_file)\n",
        "                        # Open image and apply transformations\n",
        "                        img = Image.open(frame_path).convert('L')  # Convert to grayscale\n",
        "                        img_tensor = transform(img)  # Apply transforms\n",
        "                        frames.append(img_tensor)\n",
        "\n",
        "                    # Pad or truncate sequence to fixed length (8 frames - power of 2)\n",
        "                    target_frames = 8\n",
        "                    if len(frames) < target_frames:\n",
        "                        # Pad with zeros if sequence is too short\n",
        "                        for _ in range(target_frames - len(frames)):\n",
        "                            frames.append(torch.zeros_like(frames[0]))\n",
        "                    elif len(frames) > target_frames:\n",
        "                        # Truncate if sequence is too long\n",
        "                        frames = frames[:target_frames]\n",
        "\n",
        "                    # Stack frames along a new dimension\n",
        "                    sequence_tensor = torch.stack(frames)  # Shape: [frames, channels, height, width]\n",
        "                    sequence_tensor = sequence_tensor.permute(1, 0, 2, 3)  # Reshape to [channels, frames, height, width]\n",
        "\n",
        "                    # Add sequence to dataset\n",
        "                    sequences.append(sequence_tensor)\n",
        "                    labels.append(gesture_map[gesture])\n",
        "                    subject_ids.append(subject_id)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    X = torch.stack(sequences)\n",
        "    y = torch.tensor(labels, dtype=torch.long)\n",
        "    print(y.size())\n",
        "    subject_ids = np.array(subject_ids)\n",
        "\n",
        "    return X, y, subject_ids\n",
        "\n",
        "\n",
        "# Basic 3D convolution block\n",
        "class Conv3DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(Conv3DBlock, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "# Dense Block for 3D DenseNet\n",
        "class DenseBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super(DenseBlock3D, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(self._make_dense_layer(in_channels + i * growth_rate, growth_rate))\n",
        "\n",
        "    def _make_dense_layer(self, in_channels, growth_rate):\n",
        "        return nn.Sequential(\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, 4 * growth_rate, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm3d(4 * growth_rate),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            new_feature = layer(torch.cat(features, 1))\n",
        "            features.append(new_feature)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "# Transition Layer for 3D DenseNet\n",
        "class TransitionLayer3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TransitionLayer3D, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.AvgPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# 3D DenseNet Model\n",
        "class DenseNet3D(nn.Module):\n",
        "    def __init__(self, growth_rate=12, block_config=(2, 4, 6), num__init__features=16,\n",
        "                 compression_rate=0.5, num_classes=4):\n",
        "        super(DenseNet3D, self).__init__()\n",
        "\n",
        "        # First convolution and pooling\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(1, num__init__features, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(num__init__features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Dense Blocks\n",
        "        num_features = num__init__features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            # Add a dense block\n",
        "            block = DenseBlock3D(\n",
        "                in_channels=num_features,\n",
        "                growth_rate=growth_rate,\n",
        "                num_layers=num_layers\n",
        "            )\n",
        "            self.features.add_module(f'denseblock{i+1}', block)\n",
        "            num_features += num_layers * growth_rate\n",
        "\n",
        "            # Add a transition layer (except after the last block)\n",
        "            if i != len(block_config) - 1:\n",
        "                out_features = int(num_features * compression_rate)\n",
        "                trans = TransitionLayer3D(num_features, out_features)\n",
        "                self.features.add_module(f'transition{i+1}', trans)\n",
        "                num_features = out_features\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm_final', nn.BatchNorm3d(num_features))\n",
        "        self.features.add_module('relu_final', nn.ReLU(inplace=True))\n",
        "\n",
        "        # Global Average Pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = self.avgpool(features)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Function to train the model for one epoch\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100. * correct / total\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Save predictions and targets for confusion matrix\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    return test_loss, test_acc, all_preds, all_targets\n",
        "\n",
        "\n",
        "def train_and_evaluate(X, y, subject_ids, model_type='densenet'):\n",
        "    \"\"\"\n",
        "    Train and evaluate the 3D-CNN model using k-fold cross-validation\n",
        "    with stratification across subjects.\n",
        "\n",
        "    Args:\n",
        "        X: Sequences of images tensor\n",
        "        y: Labels tensor\n",
        "        subject_ids: Subject IDs for cross-validation\n",
        "        model_type: Type of model to use\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary containing evaluation results\n",
        "        best_model_state: State dict of the best performing model\n",
        "    \"\"\"\n",
        "    # Check for CUDA availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize KFold cross-validator\n",
        "    n_splits = 5  # 5-fold cross-validation\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    subject_accuracies = []\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "\n",
        "    # Cross-validation parameters\n",
        "    num_epochs = 10\n",
        "    batch_size = 16\n",
        "\n",
        "    # Track the best model across all folds\n",
        "    best_overall_acc = 0\n",
        "    final_best_model_state = None\n",
        "\n",
        "    # Iterate through folds\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(X.numpy()), 1):\n",
        "        print(f\"\\n--- Fold {fold}/{n_splits} ---\")\n",
        "\n",
        "        # Create PyTorch datasets and dataloaders\n",
        "        train_dataset = HandGestureDataset(X[train_idx], y[train_idx])\n",
        "        test_dataset = HandGestureDataset(X[test_idx], y[test_idx])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Create the model\n",
        "        model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(device)\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=0.01,\n",
        "            steps_per_epoch=len(train_loader),\n",
        "            epochs=num_epochs\n",
        "        )\n",
        "\n",
        "        # Variables for early stopping\n",
        "        best_test_acc = 0\n",
        "        best_model_state = None\n",
        "        patience = 15\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            # Train for one epoch\n",
        "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "            # Evaluate on test set\n",
        "            test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "            # Print progress\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}: '\n",
        "                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "                  f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "            # Check for improvement\n",
        "            if test_acc > best_test_acc:\n",
        "                best_test_acc = test_acc\n",
        "                best_model_state = model.state_dict().copy()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Load best model for final evaluation\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "        # Final evaluation\n",
        "        _, final_test_acc, y_pred, y_true = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "        # Store results\n",
        "        subject_accuracies.append(final_test_acc / 100.0)\n",
        "        all_y_true.extend(y_true)\n",
        "        all_y_pred.extend(y_pred)\n",
        "\n",
        "        print(f\"Fold {fold} Final Test Accuracy: {final_test_acc:.2f}%\")\n",
        "\n",
        "        # Keep track of the best model across all folds\n",
        "        if final_test_acc > best_overall_acc:\n",
        "            best_overall_acc = final_test_acc\n",
        "            final_best_model_state = best_model_state.copy()\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    overall_accuracy = np.mean(subject_accuracies)\n",
        "    conf_matrix = confusion_matrix(all_y_true, all_y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results = {\n",
        "        'subject_accuracies': subject_accuracies,\n",
        "        'overall_accuracy': overall_accuracy,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'y_true': all_y_true,\n",
        "        'y_pred': all_y_pred\n",
        "    }\n",
        "\n",
        "    return results, final_best_model_state\n",
        "# Function to visualize results\n",
        "def visualize_results(results, model_name='densenet'):\n",
        "    \"\"\"\n",
        "    Visualize the cross-validation results.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary containing evaluation results\n",
        "        model_name: Name of the model\n",
        "    \"\"\"\n",
        "    # Plot subject accuracies\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(len(results['subject_accuracies'])), results['subject_accuracies'])\n",
        "    plt.axhline(y=results['overall_accuracy'], color='r', linestyle='-',\n",
        "                label=f\"Overall: {results['overall_accuracy']:.4f}\")\n",
        "    plt.xlabel('Subject')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Leave-One-Subject-Out Cross-Validation Results ({model_name})')\n",
        "    plt.legend()\n",
        "    plt.ylim(0, 1)\n",
        "    plt.savefig(f'subject_accuracies_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    gesture_names = ['Down', 'Left', 'Right', 'Up']\n",
        "    conf_matrix = results['confusion_matrix']\n",
        "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    sns.heatmap(conf_matrix_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=gesture_names, yticklabels=gesture_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix ({model_name})')\n",
        "    plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Main execution\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Path to your dataset\n",
        "data_path = r\"../Dataset/STMM/image\"\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "X, y, subject_ids = load_dataset(data_path)\n",
        "\n",
        "# Print dataset info\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of sequences: {len(X)}\")\n",
        "print(f\"Number of subjects: {len(np.unique(subject_ids))}\")\n",
        "\n",
        "# Run with DenseNet model\n",
        "print(\"\\n--- Training with 3D DenseNet ---\")\n",
        "densenet_results, best_model_state = train_and_evaluate(X, y, subject_ids, model_type='densenet')\n",
        "\n",
        "# Print overall results\n",
        "print(\"\\n--- Overall Results ---\")\n",
        "print(f\"DenseNet Overall Accuracy: {densenet_results['overall_accuracy']:.4f}\")\n",
        "\n",
        "# Visualize results\n",
        "print(\"\\nGenerating visualization plots...\")\n",
        "visualize_results(densenet_results, 'densenet')\n",
        "\n",
        "# Save the model\n",
        "print(\"\\nSaving model...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(device)\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Save the model state dictionary\n",
        "torch.save(model.state_dict(), 'hand_gesture_model.pth')\n",
        "print(\"Model saved successfully to 'hand_gesture_model.pth'\")\n",
        "\n",
        "print(\"\\nDone! Results saved as PNG files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2739c45a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "2739c45a",
        "outputId": "7b936f5b-405c-4ac9-bf9e-062091f66269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "DenseNet3D.__init__() got an unexpected keyword argument 'block_config'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fa043e9d5db6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results saved to prediction_results.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-fa043e9d5db6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;31m# Initialize the model with the same architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m#model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num_init_features=16).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_init_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# Load the trained model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;31m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() got an unexpected keyword argument '{next(iter(kwargs))}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DenseNet3D.__init__() got an unexpected keyword argument 'block_config'"
          ]
        }
      ],
      "source": [
        "#prediction on sample sequences (unseen data)\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the same model architecture as used during training\n",
        "class Conv3DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(Conv3DBlock, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class DenseBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super(DenseBlock3D, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(self._make_dense_layer(in_channels + i * growth_rate, growth_rate))\n",
        "\n",
        "    def _make_dense_layer(self, in_channels, growth_rate):\n",
        "        return nn.Sequential(\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, 4 * growth_rate, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm3d(4 * growth_rate),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            new_feature = layer(torch.cat(features, 1))\n",
        "            features.append(new_feature)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class TransitionLayer3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TransitionLayer3D, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.AvgPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class DenseNet3D(nn.Module):\n",
        "    def __init__(self, growth_rate=12, block_config=(2, 4, 4), num__init__features=16,\n",
        "                 compression_rate=0.5, num_classes=4):\n",
        "        super(DenseNet3D, self).__init__()\n",
        "\n",
        "        # First convolution and pooling\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(1, num__init__features, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(num__init__features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Dense Blocks\n",
        "        num_features = num__init__features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            # Add a dense block\n",
        "            block = DenseBlock3D(\n",
        "                in_channels=num_features,\n",
        "                growth_rate=growth_rate,\n",
        "                num_layers=num_layers\n",
        "            )\n",
        "            self.features.add_module(f'denseblock{i+1}', block)\n",
        "            num_features += num_layers * growth_rate\n",
        "\n",
        "            # Add a transition layer (except after the last block)\n",
        "            if i != len(block_config) - 1:\n",
        "                out_features = int(num_features * compression_rate)\n",
        "                trans = TransitionLayer3D(num_features, out_features)\n",
        "                self.features.add_module(f'transition{i+1}', trans)\n",
        "                num_features = out_features\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm_final', nn.BatchNorm3d(num_features))\n",
        "        self.features.add_module('relu_final', nn.ReLU(inplace=True))\n",
        "\n",
        "        # Global Average Pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = self.avgpool(features)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Function to preprocess and load a single sequence\n",
        "def preprocess_sequence(sequence_folder_path):\n",
        "    \"\"\"\n",
        "    Preprocesses a single sequence of images from a folder.\n",
        "\n",
        "    Args:\n",
        "        sequence_folder_path: Path to folder containing sequence frames\n",
        "\n",
        "    Returns:\n",
        "        Tensor with shape [1, C, F, H, W] (batch, channels, frames, height, width)\n",
        "    \"\"\"\n",
        "    # Same transforms as used during training\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),  # Using power of 2 dimensions for better downsampling\n",
        "        transforms.ToTensor(),  # Converts to [0,1] range and changes to CxHxW format\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] range\n",
        "    ])\n",
        "\n",
        "    # Load frames and sort them\n",
        "    frame_files = sorted([f for f in os.listdir(sequence_folder_path)\n",
        "                         if f.endswith('.jpg') or f.endswith('.png')])\n",
        "\n",
        "    # Process frames\n",
        "    frames = []\n",
        "    for frame_file in frame_files:\n",
        "        frame_path = os.path.join(sequence_folder_path, frame_file)\n",
        "        # Open image and apply transformations\n",
        "        img = Image.open(frame_path).convert('L')  # Convert to grayscale\n",
        "        img_tensor = transform(img)  # Apply transforms\n",
        "        frames.append(img_tensor)\n",
        "\n",
        "    # Pad or truncate sequence to fixed length (8 frames - power of 2)\n",
        "    target_frames = 8\n",
        "    if len(frames) < target_frames:\n",
        "        # Pad with zeros if sequence is too short\n",
        "        for _ in range(target_frames - len(frames)):\n",
        "            frames.append(torch.zeros_like(frames[0]))\n",
        "    elif len(frames) > target_frames:\n",
        "        # Truncate if sequence is too long\n",
        "        frames = frames[:target_frames]\n",
        "\n",
        "    # Stack frames along a new dimension\n",
        "    sequence_tensor = torch.stack(frames)  # Shape: [frames, channels, height, width]\n",
        "    sequence_tensor = sequence_tensor.permute(1, 0, 2, 3)  # Reshape to [channels, frames, height, width]\n",
        "\n",
        "    # Add batch dimension\n",
        "    sequence_tensor = sequence_tensor.unsqueeze(0)  # Shape: [1, channels, frames, height, width]\n",
        "\n",
        "    return sequence_tensor\n",
        "\n",
        "\n",
        "# Function to predict on a single sequence\n",
        "def predict_sequence(model, sequence_tensor, device):\n",
        "    \"\"\"\n",
        "    Makes a prediction on a single sequence.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        sequence_tensor: Preprocessed sequence tensor\n",
        "        device: Device to use for computation\n",
        "\n",
        "    Returns:\n",
        "        predicted_class: Predicted class index\n",
        "        confidence: Confidence score (probability)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sequence_tensor = sequence_tensor.to(device)\n",
        "        outputs = model(sequence_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        confidence, predicted = torch.max(probabilities, 1)\n",
        "        return predicted.item(), confidence.item()\n",
        "\n",
        "\n",
        "# Function to predict all sequences in a directory\n",
        "def predict_all_sequences(model, sequences_dir, device):\n",
        "    \"\"\"\n",
        "    Predicts classes for all sequences in the given directory.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        sequences_dir: Directory containing sequence folders\n",
        "        device: Device to use for computation\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with sequence names as keys and predictions as values\n",
        "    \"\"\"\n",
        "    # Map numerical labels to gesture names\n",
        "    label_map = {0: 'down', 1: 'left', 2: 'right', 3: 'up'}\n",
        "\n",
        "    results = {}\n",
        "    sequence_folders = sorted([f for f in os.listdir(sequences_dir)\n",
        "                              if os.path.isdir(os.path.join(sequences_dir, f))])\n",
        "\n",
        "    for sequence_folder in sequence_folders:\n",
        "        sequence_path = os.path.join(sequences_dir, sequence_folder)\n",
        "\n",
        "        # Preprocess the sequence\n",
        "        sequence_tensor = preprocess_sequence(sequence_path)\n",
        "\n",
        "        # Make prediction\n",
        "        predicted_class, confidence = predict_sequence(model, sequence_tensor, device)\n",
        "\n",
        "        # Store result\n",
        "        results[sequence_folder] = {\n",
        "            'predicted_class': predicted_class,\n",
        "            'gesture_name': label_map[predicted_class],\n",
        "            'confidence': confidence\n",
        "        }\n",
        "\n",
        "        print(f\"Sequence: {sequence_folder}, Predicted: {label_map[predicted_class]}, Confidence: {confidence:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to visualize predictions\n",
        "def visualize_predictions(results, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualizes prediction results.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary with prediction results\n",
        "        save_path: Path to save the visualization\n",
        "    \"\"\"\n",
        "    sequences = list(results.keys())\n",
        "    confidences = [results[seq]['confidence'] for seq in sequences]\n",
        "    gestures = [results[seq]['gesture_name'] for seq in sequences]\n",
        "\n",
        "    # Set colors based on gesture\n",
        "    colors = {'down': 'blue', 'left': 'green', 'right': 'red', 'up': 'purple'}\n",
        "    bar_colors = [colors[gesture] for gesture in gestures]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(sequences, confidences, color=bar_colors)\n",
        "\n",
        "    # Add gesture labels on top of bars\n",
        "    for bar, gesture in zip(bars, gestures):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                 gesture, ha='center', fontsize=9)\n",
        "\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.xlabel('Sequence')\n",
        "    plt.ylabel('Confidence')\n",
        "    plt.title('Hand Gesture Predictions')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Create legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor=color, label=gesture)\n",
        "                      for gesture, color in colors.items()]\n",
        "    plt.legend(handles=legend_elements)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Visualization saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Main function to load model and predict\n",
        "def main():\n",
        "    # Path to the saved model\n",
        "    model_path = 'hand_gesture_model.pth'\n",
        "\n",
        "    # Path to the directory containing sample sequences\n",
        "    sequences_dir = 'D/sample_sequence'\n",
        "\n",
        "    # Check if GPU is available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize the model with the same architecture\n",
        "    #model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(device)\n",
        "    model = DenseNet3D(block_config=(2, 4, 4), num__init__features=16).to(device)\n",
        "\n",
        "    # Load the trained model weights\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        print(f\"Model loaded successfully from {model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return\n",
        "\n",
        "    # Predict all sequences\n",
        "    print(\"\\nMaking predictions on sample sequences...\")\n",
        "    results = predict_all_sequences(model, sequences_dir, device)\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\nVisualizing predictions...\")\n",
        "    visualize_predictions(results, save_path='predictions_visualization.png')\n",
        "\n",
        "    # Save results to file\n",
        "    import json\n",
        "    with open('prediction_results.json', 'w') as f:\n",
        "        # Convert results to serializable format\n",
        "        serializable_results = {}\n",
        "        for key, value in results.items():\n",
        "            serializable_results[key] = {\n",
        "                'predicted_class': int(value['predicted_class']),\n",
        "                'gesture_name': value['gesture_name'],\n",
        "                'confidence': float(value['confidence'])\n",
        "            }\n",
        "        json.dump(serializable_results, f, indent=4)\n",
        "\n",
        "    print(\"Results saved to prediction_results.json\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a12481",
      "metadata": {
        "id": "f2a12481"
      },
      "outputs": [],
      "source": [
        "\"\"\"Best Live Prediction UpTill Now\"\"\"\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from collections import deque\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Recreate the DenseNet3D model to match your training architecture\n",
        "class Conv3DBlock(nn.Module):\n",
        "    def ___init___(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(Conv3DBlock, self).___init___()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class DenseBlock3D(nn.Module):\n",
        "    def ___init___(self, in_channels, growth_rate, num_layers):\n",
        "        super(DenseBlock3D, self).___init___()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(self._make_dense_layer(in_channels + i * growth_rate, growth_rate))\n",
        "\n",
        "    def _make_dense_layer(self, in_channels, growth_rate):\n",
        "        return nn.Sequential(\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, 4 * growth_rate, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm3d(4 * growth_rate),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            new_feature = layer(torch.cat(features, 1))\n",
        "            features.append(new_feature)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class TransitionLayer3D(nn.Module):\n",
        "    def ___init___(self, in_channels, out_channels):\n",
        "        super(TransitionLayer3D, self).___init___()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.AvgPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class DenseNet3D(nn.Module):\n",
        "    def ___init___(self, growth_rate=12, block_config=(2, 4, 4), num__init__features=16,\n",
        "                 compression_rate=0.5, num_classes=4):\n",
        "        super(DenseNet3D, self).___init___()\n",
        "\n",
        "        # First convolution and pooling\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(1, num__init__features, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(num__init__features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Dense Blocks\n",
        "        num_features = num__init__features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            # Add a dense block\n",
        "            block = DenseBlock3D(\n",
        "                in_channels=num_features,\n",
        "                growth_rate=growth_rate,\n",
        "                num_layers=num_layers\n",
        "            )\n",
        "            self.features.add_module(f'denseblock{i+1}', block)\n",
        "            num_features += num_layers * growth_rate\n",
        "\n",
        "            # Add a transition layer (except after the last block)\n",
        "            if i != len(block_config) - 1:\n",
        "                out_features = int(num_features * compression_rate)\n",
        "                trans = TransitionLayer3D(num_features, out_features)\n",
        "                self.features.add_module(f'transition{i+1}', trans)\n",
        "                num_features = out_features\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm_final', nn.BatchNorm3d(num_features))\n",
        "        self.features.add_module('relu_final', nn.ReLU(inplace=True))\n",
        "\n",
        "        # Global Average Pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = self.avgpool(features)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class GestureRecognition:\n",
        "    def ___init___(self, model_path, frame_count=8, collection_time=2.0, display_fps=True):\n",
        "        # Initialize MediaPipe Hands\n",
        "        self.mp_hands = mp.solutions.hands\n",
        "        self.hands = self.mp_hands.Hands(\n",
        "            static_image_mode=False,\n",
        "            max_num_hands=1,\n",
        "            min_detection_confidence=0.5,\n",
        "            min_tracking_confidence=0.5\n",
        "        )\n",
        "        self.mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "        # Load the trained model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(self.device)\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "        self.model.eval()\n",
        "\n",
        "        # Gesture classes\n",
        "        self.gesture_classes = ['down', 'left', 'right', 'up']\n",
        "\n",
        "        # Initialize variables for frames processing\n",
        "        self.frame_count = frame_count\n",
        "        self.collection_time = collection_time  # Time in seconds to collect frames\n",
        "        self.frame_buffer = deque(maxlen=frame_count)\n",
        "        self.all_processed_frames = []  # Store all processed frames during collection\n",
        "        self.is_collecting = False\n",
        "        self.hand_detected_count = 0\n",
        "        self.min_hand_detected = 6  # Minimum number of frames that must have a hand\n",
        "\n",
        "        # Collection timing variables\n",
        "        self.collection_start_time = 0\n",
        "        self.collection_end_time = 0\n",
        "\n",
        "        # Define the same transformation pipeline as in training\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "\n",
        "        # FPS calculation\n",
        "        self.prev_frame_time = 0\n",
        "        self.new_frame_time = 0\n",
        "        self.display_fps = display_fps\n",
        "\n",
        "        # Create directory for saving processed frames\n",
        "        self.frames_dir = \"processed_frames\"\n",
        "        os.makedirs(self.frames_dir, exist_ok=True)\n",
        "\n",
        "    def preprocess_hand_frame(self, frame, results):\n",
        "        \"\"\"Extract hand from frame and preprocess it\"\"\"\n",
        "        h, w, _ = frame.shape\n",
        "\n",
        "        # If no hand is detected, return None\n",
        "        if not results.multi_hand_landmarks:\n",
        "            return None, None\n",
        "\n",
        "        # Get landmarks for the first hand detected\n",
        "        hand_landmarks = results.multi_hand_landmarks[0]\n",
        "\n",
        "        # Calculate bounding box with padding\n",
        "        x_min, x_max, y_min, y_max = w, 0, h, 0\n",
        "        for landmark in hand_landmarks.landmark:\n",
        "            x, y = int(landmark.x * w), int(landmark.y * h)\n",
        "            x_min = min(x_min, x)\n",
        "            x_max = max(x_max, x)\n",
        "            y_min = min(y_min, y)\n",
        "            y_max = max(y_max, y)\n",
        "\n",
        "        # Add padding\n",
        "        padding = 20\n",
        "        x_min = max(0, x_min - padding)\n",
        "        y_min = max(0, y_min - padding)\n",
        "        x_max = min(w, x_max + padding)\n",
        "        y_max = min(h, y_max + padding)\n",
        "\n",
        "        # Check if bounding box is valid\n",
        "        if x_min >= x_max or y_min >= y_max:\n",
        "            return None, None\n",
        "\n",
        "        # Crop the hand region\n",
        "        hand_crop = frame[y_min:y_max, x_min:x_max].copy()\n",
        "\n",
        "        # Create a copy of the original frame with hand landmarks\n",
        "        marked_frame = frame.copy()\n",
        "        self.mp_drawing.draw_landmarks(\n",
        "            marked_frame,\n",
        "            hand_landmarks,\n",
        "            self.mp_hands.HAND_CONNECTIONS\n",
        "        )\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(marked_frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "\n",
        "        # Convert to grayscale and PIL image\n",
        "        hand_crop_gray = cv2.cvtColor(hand_crop, cv2.COLOR_BGR2GRAY)\n",
        "        pil_img = Image.fromarray(hand_crop_gray)\n",
        "\n",
        "        # Apply transforms\n",
        "        img_tensor = self.transform(pil_img)\n",
        "\n",
        "        return img_tensor, marked_frame\n",
        "\n",
        "    def predict_gesture(self):\n",
        "        \"\"\"Make a prediction based on collected frames\"\"\"\n",
        "        if len(self.frame_buffer) < self.frame_count:\n",
        "            return \"Insufficient frames\", 0.0\n",
        "\n",
        "        if self.hand_detected_count < self.min_hand_detected:\n",
        "            return \"Hand not consistently detected\", 0.0\n",
        "\n",
        "        # Stack frames into a sequence\n",
        "        sequence = torch.stack(list(self.frame_buffer))  # Shape: [frames, channels, height, width]\n",
        "        sequence = sequence.permute(1, 0, 2, 3).unsqueeze(0)  # Reshape to [1, channels, frames, height, width]\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            sequence = sequence.to(self.device)\n",
        "            outputs = self.model(sequence)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            confidence, prediction = torch.max(probabilities, 1)\n",
        "\n",
        "        # Get prediction class and confidence\n",
        "        predicted_class = self.gesture_classes[prediction.item()]\n",
        "        confidence_value = confidence.item()\n",
        "\n",
        "        return predicted_class, confidence_value\n",
        "\n",
        "    def save_processed_frames(self):\n",
        "        \"\"\"Save the processed frames for visualization\"\"\"\n",
        "        # Clear previous frames\n",
        "        for file in os.listdir(self.frames_dir):\n",
        "            os.remove(os.path.join(self.frames_dir, file))\n",
        "\n",
        "        # Save current frame buffer\n",
        "        for i, frame_tensor in enumerate(self.frame_buffer):\n",
        "            # Convert tensor to numpy array for visualization\n",
        "            frame_np = frame_tensor.cpu().numpy()[0]  # Get the first channel\n",
        "            frame_np = (frame_np * 0.5 + 0.5) * 255  # Denormalize\n",
        "            frame_np = frame_np.astype(np.uint8)\n",
        "\n",
        "            # Save the frame\n",
        "            cv2.imwrite(os.path.join(self.frames_dir, f\"frame_{i+1}.jpg\"), frame_np)\n",
        "\n",
        "    def visualize_processed_frames(self, prediction, confidence):\n",
        "        \"\"\"Plot frames used for prediction\"\"\"\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        for i, frame_path in enumerate(sorted(os.listdir(self.frames_dir))):\n",
        "            plt.subplot(2, 4, i+1)\n",
        "            frame = cv2.imread(os.path.join(self.frames_dir, frame_path), cv2.IMREAD_GRAYSCALE)\n",
        "            plt.imshow(frame, cmap='gray')\n",
        "            plt.title(f\"Frame {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Prediction: {prediction.upper()} (Confidence: {confidence:.2f})\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"prediction_frames.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def select_evenly_spaced_frames(self):\n",
        "        \"\"\"Select evenly spaced frames from all collected frames\"\"\"\n",
        "        total_frames = len(self.all_processed_frames)\n",
        "        if total_frames <= self.frame_count:\n",
        "            return self.all_processed_frames\n",
        "\n",
        "        # Calculate indices for evenly spaced frames\n",
        "        indices = np.linspace(0, total_frames - 1, self.frame_count, dtype=int)\n",
        "        selected_frames = [self.all_processed_frames[i] for i in indices]\n",
        "        return selected_frames\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Run the gesture recognition system\"\"\"\n",
        "        cap = cv2.VideoCapture(0)\n",
        "\n",
        "        # Status variables\n",
        "        collecting_status = \"Press 'c' to start collecting frames\"\n",
        "        prediction_result = \"No prediction yet\"\n",
        "        confidence = 0.0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Calculate FPS\n",
        "            if self.display_fps:\n",
        "                self.new_frame_time = time.time()\n",
        "                fps = 1/(self.new_frame_time - self.prev_frame_time) if self.prev_frame_time > 0 else 0\n",
        "                self.prev_frame_time = self.new_frame_time\n",
        "                cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "            # Convert the BGR image to RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = self.hands.process(rgb_frame)\n",
        "\n",
        "            # Draw hand landmarks on the frame\n",
        "            if results.multi_hand_landmarks:\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    self.mp_drawing.draw_landmarks(\n",
        "                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            # Display collection status\n",
        "            cv2.putText(frame, collecting_status, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "            # Display prediction result\n",
        "            cv2.putText(frame, f\"Prediction: {prediction_result}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "            if confidence > 0:\n",
        "                cv2.putText(frame, f\"Confidence: {confidence:.2f}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "            # If collecting frames\n",
        "            if self.is_collecting:\n",
        "                current_time = time.time()\n",
        "                elapsed_time = current_time - self.collection_start_time\n",
        "\n",
        "                # Process the current frame\n",
        "                processed_frame, marked_frame = self.preprocess_hand_frame(frame, results)\n",
        "\n",
        "                if processed_frame is not None:\n",
        "                    self.all_processed_frames.append(processed_frame)\n",
        "                    self.hand_detected_count += 1\n",
        "                    if marked_frame is not None:\n",
        "                        frame = marked_frame\n",
        "\n",
        "                # Update collection status with time remaining\n",
        "                time_left = max(0, self.collection_time - elapsed_time)\n",
        "                collecting_status = f\"Collecting: {elapsed_time:.1f}s / {self.collection_time:.1f}s\"\n",
        "\n",
        "                # Add a progress bar\n",
        "                progress = int(min(elapsed_time / self.collection_time, 1.0) * 200)\n",
        "                cv2.rectangle(frame, (10, 150), (210, 170), (0, 0, 0), -1)\n",
        "                cv2.rectangle(frame, (10, 150), (10 + progress, 170), (0, 255, 0), -1)\n",
        "\n",
        "                # If collection time is over\n",
        "                if elapsed_time >= self.collection_time:\n",
        "                    self.is_collecting = False\n",
        "\n",
        "                    # Select evenly spaced frames from all collected\n",
        "                    selected_frames = self.select_evenly_spaced_frames()\n",
        "\n",
        "                    # Update frame buffer with selected frames\n",
        "                    self.frame_buffer = deque(selected_frames, maxlen=self.frame_count)\n",
        "\n",
        "                    # Make prediction\n",
        "                    prediction_result, confidence = self.predict_gesture()\n",
        "                    collecting_status = \"Press 'c' to collect new frames\"\n",
        "\n",
        "                    # Save and visualize processed frames\n",
        "                    self.save_processed_frames()\n",
        "                    self.visualize_processed_frames(prediction_result, confidence)\n",
        "\n",
        "            # Display the resulting frame\n",
        "            cv2.imshow('Hand Gesture Recognition', frame)\n",
        "\n",
        "            # Key handling\n",
        "            key = cv2.waitKey(1) & 0xFF\n",
        "            if key == ord('q'):\n",
        "                break\n",
        "            elif key == ord('c'):\n",
        "                # Start collecting frames\n",
        "                self.is_collecting = True\n",
        "                self.all_processed_frames = []\n",
        "                self.frame_buffer.clear()\n",
        "                self.hand_detected_count = 0\n",
        "                self.collection_start_time = time.time()\n",
        "                collecting_status = \"Collecting started...\"\n",
        "\n",
        "        # Release the capture and close windows\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with the path to your trained model\n",
        "    model_path = \"hand_gesture_model.pth\"\n",
        "\n",
        "    # Initialize and run the gesture recognition system\n",
        "    # Set collection_time to 2.0 seconds\n",
        "    gesture_recognition = GestureRecognition(model_path, collection_time=1.0)\n",
        "    gesture_recognition.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

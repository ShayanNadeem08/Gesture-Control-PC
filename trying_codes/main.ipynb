{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Using device: cpu\n",
      "Defining 3D DenseNet model\n",
      "Training model\n",
      "Using 5 fold cross validation\n",
      "Fold 1/5:\n",
      "torch.Size([16, 8, 3, 64, 64]) torch.Size([16])\n",
      "torch.Size([16, 8, 3, 64, 64]) torch.Size([16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PMLS\\FYP\\Gesture-Control-PC\\codes\\main.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/main.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m CHECKING:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/main.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/main.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         densenet_results, best_model_state, model \u001b[39m=\u001b[39m kfold_train_and_validate(model, train_loader, device, num_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, num_kfold_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/main.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PMLS/FYP/Gesture-Control-PC/codes/main.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     densenet_results, best_model_state, model \u001b[39m=\u001b[39m kfold_train_and_validate(model, train_loader, device, num_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, num_kfold_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\FYP\\Gesture-Control-PC\\codes\\training.py:141\u001b[0m, in \u001b[0;36mkfold_train_and_validate\u001b[1;34m(model, train_loader, device, num_epochs, num_kfold_splits, batch_size)\u001b[0m\n\u001b[0;32m    138\u001b[0m train_loss, train_acc, valid_loss, valid_acc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m4\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[39m# Train for one epoch\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m train_epoch(model, train_loader, train_idx, criterion, optimizer, device)\n\u001b[0;32m    143\u001b[0m \u001b[39m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    144\u001b[0m valid_loss, valid_acc, _, _ \u001b[39m=\u001b[39m validate(model, train_loader, valid_idx, criterion, device)\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\FYP\\Gesture-Control-PC\\codes\\training.py:31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, train_idx_list, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     30\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m---> 31\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m#loss.backward()\u001b[39;00m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mcross_entropy(\n\u001b[0;32m   1294\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         target,\n\u001b[0;32m   1296\u001b[0m         weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[0;32m   1297\u001b[0m         ignore_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index,\n\u001b[0;32m   1298\u001b[0m         reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction,\n\u001b[0;32m   1299\u001b[0m         label_smoothing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1300\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3480\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m   3481\u001b[0m     target,\n\u001b[0;32m   3482\u001b[0m     weight,\n\u001b[0;32m   3483\u001b[0m     _Reduction\u001b[39m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3484\u001b[0m     ignore_index,\n\u001b[0;32m   3485\u001b[0m     label_smoothing,\n\u001b[0;32m   3486\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
     ]
    }
   ],
   "source": [
    "\"\"\" Training code \"\"\"\n",
    "import sys\n",
    "import torch\n",
    "# Import from local files\n",
    "from global_defines import DATASET_PATH as data_path\n",
    "from load_dataset import load_dataset\n",
    "from model import DenseNet3D\n",
    "from training import kfold_train_and_validate\n",
    "from log_visualize import visualize_results\n",
    "sys.path.insert(1, \"../external_modules/\")\n",
    "import timestamp\n",
    "from global_defines import CHECKING\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "print(\"Loading dataset\")\n",
    "datasets, loaders = load_dataset(data_path, [0.7,0.3], batch_size=16)\n",
    "train_dataset, test_dataset = datasets\n",
    "train_loader, test_loader = loaders\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the model\n",
    "print(\"Defining 3D DenseNet model\")\n",
    "model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(device)\n",
    "\n",
    "# Run with DenseNet model\n",
    "print(\"Training model\")\n",
    "if CHECKING:\n",
    "    with torch.no_grad():\n",
    "        densenet_results, best_model_state, model = kfold_train_and_validate(model, train_loader, device, num_epochs=10, num_kfold_splits=5, batch_size=16)\n",
    "else:\n",
    "    densenet_results, best_model_state, model = kfold_train_and_validate(model, train_loader, device, num_epochs=10, num_kfold_splits=5, batch_size=16)\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Accuracy: {densenet_results['overall_accuracy']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "save_path = \"../model/DENSENET_\"+timestamp+\".model\"\n",
    "print(\"\\nSaving model to {savepath}\")\n",
    "torch.save(model, save_path)\n",
    "print(\"\\nSaving model state\")\n",
    "torch.save(best_model_state, savepath+\"state\")\n",
    "\n",
    "# Visualize results\n",
    "print(\"\\nGenerating visualization plots\")\n",
    "visualize_results(densenet_results, 'densenet')\n",
    "\n",
    "print(\"\\nCompleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f82cea-3740-4ea3-8263-5b6e7966d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "import sys\n",
    "# import numpy as np\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# Import from local files\n",
    "from global_defines import DATASET_PATH as data_path\n",
    "#from load_dataset_old import HandGestureDataset, load_dataset\n",
    "from load_dataset import load_dataset\n",
    "from model import DenseNet3D\n",
    "from training import train_and_validate\n",
    "from log_visualize import print_log, visualize_results\n",
    "sys.path.insert(1, \"../external_modules/\")\n",
    "import timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739c45a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "2739c45a",
    "outputId": "7b936f5b-405c-4ac9-bf9e-062091f66269"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess and load a single sequence\n",
    "def preprocess_sequence(sequence_folder_path):\n",
    "    \"\"\"\n",
    "    Preprocesses a single sequence of images from a folder.\n",
    "\n",
    "    Args:\n",
    "        sequence_folder_path: Path to folder containing sequence frames\n",
    "\n",
    "    Returns:\n",
    "        Tensor with shape [1, C, F, H, W] (batch, channels, frames, height, width)\n",
    "    \"\"\"\n",
    "    # Same transforms as used during training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # Using power of 2 dimensions for better downsampling\n",
    "        transforms.ToTensor(),  # Converts to [0,1] range and changes to CxHxW format\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] range\n",
    "    ])\n",
    "\n",
    "    # Load frames and sort them\n",
    "    frame_files = sorted([f for f in os.listdir(sequence_folder_path)\n",
    "                         if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "    # Process frames\n",
    "    frames = []\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(sequence_folder_path, frame_file)\n",
    "        # Open image and apply transformations\n",
    "        img = Image.open(frame_path).convert('L')  # Convert to grayscale\n",
    "        img_tensor = transform(img)  # Apply transforms\n",
    "        frames.append(img_tensor)\n",
    "\n",
    "    # Pad or truncate sequence to fixed length (8 frames - power of 2)\n",
    "    target_frames = 8\n",
    "    if len(frames) < target_frames:\n",
    "        # Pad with zeros if sequence is too short\n",
    "        for _ in range(target_frames - len(frames)):\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "    elif len(frames) > target_frames:\n",
    "        # Truncate if sequence is too long\n",
    "        frames = frames[:target_frames]\n",
    "\n",
    "    # Stack frames along a new dimension\n",
    "    sequence_tensor = torch.stack(frames)  # Shape: [frames, channels, height, width]\n",
    "    sequence_tensor = sequence_tensor.permute(1, 0, 2, 3)  # Reshape to [channels, frames, height, width]\n",
    "\n",
    "    # Add batch dimension\n",
    "    sequence_tensor = sequence_tensor.unsqueeze(0)  # Shape: [1, channels, frames, height, width]\n",
    "\n",
    "    return sequence_tensor\n",
    "\n",
    "\n",
    "# Function to predict on a single sequence\n",
    "def predict_sequence(model, sequence_tensor, device):\n",
    "    \"\"\"\n",
    "    Makes a prediction on a single sequence.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        sequence_tensor: Preprocessed sequence tensor\n",
    "        device: Device to use for computation\n",
    "\n",
    "    Returns:\n",
    "        predicted_class: Predicted class index\n",
    "        confidence: Confidence score (probability)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence_tensor = sequence_tensor.to(device)\n",
    "        outputs = model(sequence_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "        return predicted.item(), confidence.item()\n",
    "\n",
    "\n",
    "# Function to predict all sequences in a directory\n",
    "def predict_all_sequences(model, sequences_dir, device):\n",
    "    \"\"\"\n",
    "    Predicts classes for all sequences in the given directory.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        sequences_dir: Directory containing sequence folders\n",
    "        device: Device to use for computation\n",
    "\n",
    "    Returns:\n",
    "        results: Dictionary with sequence names as keys and predictions as values\n",
    "    \"\"\"\n",
    "    # Map numerical labels to gesture names\n",
    "    label_map = {0: 'down', 1: 'left', 2: 'right', 3: 'up'}\n",
    "\n",
    "    results = {}\n",
    "    sequence_folders = sorted([f for f in os.listdir(sequences_dir)\n",
    "                              if os.path.isdir(os.path.join(sequences_dir, f))])\n",
    "\n",
    "    for sequence_folder in sequence_folders:\n",
    "        sequence_path = os.path.join(sequences_dir, sequence_folder)\n",
    "\n",
    "        # Preprocess the sequence\n",
    "        sequence_tensor = preprocess_sequence(sequence_path)\n",
    "\n",
    "        # Make prediction\n",
    "        predicted_class, confidence = predict_sequence(model, sequence_tensor, device)\n",
    "\n",
    "        # Store result\n",
    "        results[sequence_folder] = {\n",
    "            'predicted_class': predicted_class,\n",
    "            'gesture_name': label_map[predicted_class],\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "        print(f\"Sequence: {sequence_folder}, Predicted: {label_map[predicted_class]}, Confidence: {confidence:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_predictions(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes prediction results.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary with prediction results\n",
    "        save_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    sequences = list(results.keys())\n",
    "    confidences = [results[seq]['confidence'] for seq in sequences]\n",
    "    gestures = [results[seq]['gesture_name'] for seq in sequences]\n",
    "\n",
    "    # Set colors based on gesture\n",
    "    colors = {'down': 'blue', 'left': 'green', 'right': 'red', 'up': 'purple'}\n",
    "    bar_colors = [colors[gesture] for gesture in gestures]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(sequences, confidences, color=bar_colors)\n",
    "\n",
    "    # Add gesture labels on top of bars\n",
    "    for bar, gesture in zip(bars, gestures):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 gesture, ha='center', fontsize=9)\n",
    "\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xlabel('Sequence')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title('Hand Gesture Predictions')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Create legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color, label=gesture)\n",
    "                      for gesture, color in colors.items()]\n",
    "    plt.legend(handles=legend_elements)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main function to load model and predict\n",
    "def main():\n",
    "    # Path to the saved model\n",
    "    model_path = 'hand_gesture_model.pth'\n",
    "\n",
    "    # Path to the directory containing sample sequences\n",
    "    sequences_dir = 'D/sample_sequence'\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize the model with the same architecture\n",
    "    #model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(device)\n",
    "    model = DenseNet3D(block_config=(2, 4, 4), num__init__features=16).to(device)\n",
    "\n",
    "    # Load the trained model weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Predict all sequences\n",
    "    print(\"\\nMaking predictions on sample sequences...\")\n",
    "    results = predict_all_sequences(model, sequences_dir, device)\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"\\nVisualizing predictions...\")\n",
    "    visualize_predictions(results, save_path='predictions_visualization.png')\n",
    "\n",
    "    # Save results to file\n",
    "    import json\n",
    "    with open('prediction_results.json', 'w') as f:\n",
    "        # Convert results to serializable format\n",
    "        serializable_results = {}\n",
    "        for key, value in results.items():\n",
    "            serializable_results[key] = {\n",
    "                'predicted_class': int(value['predicted_class']),\n",
    "                'gesture_name': value['gesture_name'],\n",
    "                'confidence': float(value['confidence'])\n",
    "            }\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "\n",
    "    print(\"Results saved to prediction_results.json\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a12481",
   "metadata": {
    "id": "f2a12481"
   },
   "outputs": [],
   "source": [
    "\"\"\"Best Live Prediction UpTill Now\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from collections import deque\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Recreate the DenseNet3D model to match your training architecture\n",
    "class Conv3DBlock(nn.Module):\n",
    "    def ___init___(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv3DBlock, self).___init___()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class DenseBlock3D(nn.Module):\n",
    "    def ___init___(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock3D, self).___init___()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(self._make_dense_layer(in_channels + i * growth_rate, growth_rate))\n",
    "\n",
    "    def _make_dense_layer(self, in_channels, growth_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, 4 * growth_rate, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm3d(4 * growth_rate),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            new_feature = layer(torch.cat(features, 1))\n",
    "            features.append(new_feature)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class TransitionLayer3D(nn.Module):\n",
    "    def ___init___(self, in_channels, out_channels):\n",
    "        super(TransitionLayer3D, self).___init___()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DenseNet3D(nn.Module):\n",
    "    def ___init___(self, growth_rate=12, block_config=(2, 4, 4), num__init__features=16,\n",
    "                 compression_rate=0.5, num_classes=4):\n",
    "        super(DenseNet3D, self).___init___()\n",
    "\n",
    "        # First convolution and pooling\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, num__init__features, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(num__init__features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Dense Blocks\n",
    "        num_features = num__init__features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Add a dense block\n",
    "            block = DenseBlock3D(\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                num_layers=num_layers\n",
    "            )\n",
    "            self.features.add_module(f'denseblock{i+1}', block)\n",
    "            num_features += num_layers * growth_rate\n",
    "\n",
    "            # Add a transition layer (except after the last block)\n",
    "            if i != len(block_config) - 1:\n",
    "                out_features = int(num_features * compression_rate)\n",
    "                trans = TransitionLayer3D(num_features, out_features)\n",
    "                self.features.add_module(f'transition{i+1}', trans)\n",
    "                num_features = out_features\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm_final', nn.BatchNorm3d(num_features))\n",
    "        self.features.add_module('relu_final', nn.ReLU(inplace=True))\n",
    "\n",
    "        # Global Average Pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.avgpool(features)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GestureRecognition:\n",
    "    def ___init___(self, model_path, frame_count=8, collection_time=2.0, display_fps=True):\n",
    "        # Initialize MediaPipe Hands\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "        # Load the trained model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "\n",
    "        # Gesture classes\n",
    "        self.gesture_classes = ['down', 'left', 'right', 'up']\n",
    "\n",
    "        # Initialize variables for frames processing\n",
    "        self.frame_count = frame_count\n",
    "        self.collection_time = collection_time  # Time in seconds to collect frames\n",
    "        self.frame_buffer = deque(maxlen=frame_count)\n",
    "        self.all_processed_frames = []  # Store all processed frames during collection\n",
    "        self.is_collecting = False\n",
    "        self.hand_detected_count = 0\n",
    "        self.min_hand_detected = 6  # Minimum number of frames that must have a hand\n",
    "\n",
    "        # Collection timing variables\n",
    "        self.collection_start_time = 0\n",
    "        self.collection_end_time = 0\n",
    "\n",
    "        # Define the same transformation pipeline as in training\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "        # FPS calculation\n",
    "        self.prev_frame_time = 0\n",
    "        self.new_frame_time = 0\n",
    "        self.display_fps = display_fps\n",
    "\n",
    "        # Create directory for saving processed frames\n",
    "        self.frames_dir = \"processed_frames\"\n",
    "        os.makedirs(self.frames_dir, exist_ok=True)\n",
    "\n",
    "    def preprocess_hand_frame(self, frame, results):\n",
    "        \"\"\"Extract hand from frame and preprocess it\"\"\"\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # If no hand is detected, return None\n",
    "        if not results.multi_hand_landmarks:\n",
    "            return None, None\n",
    "\n",
    "        # Get landmarks for the first hand detected\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Calculate bounding box with padding\n",
    "        x_min, x_max, y_min, y_max = w, 0, h, 0\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "            x_min = min(x_min, x)\n",
    "            x_max = max(x_max, x)\n",
    "            y_min = min(y_min, y)\n",
    "            y_max = max(y_max, y)\n",
    "\n",
    "        # Add padding\n",
    "        padding = 20\n",
    "        x_min = max(0, x_min - padding)\n",
    "        y_min = max(0, y_min - padding)\n",
    "        x_max = min(w, x_max + padding)\n",
    "        y_max = min(h, y_max + padding)\n",
    "\n",
    "        # Check if bounding box is valid\n",
    "        if x_min >= x_max or y_min >= y_max:\n",
    "            return None, None\n",
    "\n",
    "        # Crop the hand region\n",
    "        hand_crop = frame[y_min:y_max, x_min:x_max].copy()\n",
    "\n",
    "        # Create a copy of the original frame with hand landmarks\n",
    "        marked_frame = frame.copy()\n",
    "        self.mp_drawing.draw_landmarks(\n",
    "            marked_frame,\n",
    "            hand_landmarks,\n",
    "            self.mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(marked_frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "        # Convert to grayscale and PIL image\n",
    "        hand_crop_gray = cv2.cvtColor(hand_crop, cv2.COLOR_BGR2GRAY)\n",
    "        pil_img = Image.fromarray(hand_crop_gray)\n",
    "\n",
    "        # Apply transforms\n",
    "        img_tensor = self.transform(pil_img)\n",
    "\n",
    "        return img_tensor, marked_frame\n",
    "\n",
    "    def predict_gesture(self):\n",
    "        \"\"\"Make a prediction based on collected frames\"\"\"\n",
    "        if len(self.frame_buffer) < self.frame_count:\n",
    "            return \"Insufficient frames\", 0.0\n",
    "\n",
    "        if self.hand_detected_count < self.min_hand_detected:\n",
    "            return \"Hand not consistently detected\", 0.0\n",
    "\n",
    "        # Stack frames into a sequence\n",
    "        sequence = torch.stack(list(self.frame_buffer))  # Shape: [frames, channels, height, width]\n",
    "        sequence = sequence.permute(1, 0, 2, 3).unsqueeze(0)  # Reshape to [1, channels, frames, height, width]\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            sequence = sequence.to(self.device)\n",
    "            outputs = self.model(sequence)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidence, prediction = torch.max(probabilities, 1)\n",
    "\n",
    "        # Get prediction class and confidence\n",
    "        predicted_class = self.gesture_classes[prediction.item()]\n",
    "        confidence_value = confidence.item()\n",
    "\n",
    "        return predicted_class, confidence_value\n",
    "\n",
    "    def save_processed_frames(self):\n",
    "        \"\"\"Save the processed frames for visualization\"\"\"\n",
    "        # Clear previous frames\n",
    "        for file in os.listdir(self.frames_dir):\n",
    "            os.remove(os.path.join(self.frames_dir, file))\n",
    "\n",
    "        # Save current frame buffer\n",
    "        for i, frame_tensor in enumerate(self.frame_buffer):\n",
    "            # Convert tensor to numpy array for visualization\n",
    "            frame_np = frame_tensor.cpu().numpy()[0]  # Get the first channel\n",
    "            frame_np = (frame_np * 0.5 + 0.5) * 255  # Denormalize\n",
    "            frame_np = frame_np.astype(np.uint8)\n",
    "\n",
    "            # Save the frame\n",
    "            cv2.imwrite(os.path.join(self.frames_dir, f\"frame_{i+1}.jpg\"), frame_np)\n",
    "\n",
    "    def visualize_processed_frames(self, prediction, confidence):\n",
    "        \"\"\"Plot frames used for prediction\"\"\"\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        for i, frame_path in enumerate(sorted(os.listdir(self.frames_dir))):\n",
    "            plt.subplot(2, 4, i+1)\n",
    "            frame = cv2.imread(os.path.join(self.frames_dir, frame_path), cv2.IMREAD_GRAYSCALE)\n",
    "            plt.imshow(frame, cmap='gray')\n",
    "            plt.title(f\"Frame {i+1}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Prediction: {prediction.upper()} (Confidence: {confidence:.2f})\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"prediction_frames.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def select_evenly_spaced_frames(self):\n",
    "        \"\"\"Select evenly spaced frames from all collected frames\"\"\"\n",
    "        total_frames = len(self.all_processed_frames)\n",
    "        if total_frames <= self.frame_count:\n",
    "            return self.all_processed_frames\n",
    "\n",
    "        # Calculate indices for evenly spaced frames\n",
    "        indices = np.linspace(0, total_frames - 1, self.frame_count, dtype=int)\n",
    "        selected_frames = [self.all_processed_frames[i] for i in indices]\n",
    "        return selected_frames\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the gesture recognition system\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        # Status variables\n",
    "        collecting_status = \"Press 'c' to start collecting frames\"\n",
    "        prediction_result = \"No prediction yet\"\n",
    "        confidence = 0.0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Calculate FPS\n",
    "            if self.display_fps:\n",
    "                self.new_frame_time = time.time()\n",
    "                fps = 1/(self.new_frame_time - self.prev_frame_time) if self.prev_frame_time > 0 else 0\n",
    "                self.prev_frame_time = self.new_frame_time\n",
    "                cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = self.hands.process(rgb_frame)\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Display collection status\n",
    "            cv2.putText(frame, collecting_status, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Display prediction result\n",
    "            cv2.putText(frame, f\"Prediction: {prediction_result}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if confidence > 0:\n",
    "                cv2.putText(frame, f\"Confidence: {confidence:.2f}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # If collecting frames\n",
    "            if self.is_collecting:\n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - self.collection_start_time\n",
    "\n",
    "                # Process the current frame\n",
    "                processed_frame, marked_frame = self.preprocess_hand_frame(frame, results)\n",
    "\n",
    "                if processed_frame is not None:\n",
    "                    self.all_processed_frames.append(processed_frame)\n",
    "                    self.hand_detected_count += 1\n",
    "                    if marked_frame is not None:\n",
    "                        frame = marked_frame\n",
    "\n",
    "                # Update collection status with time remaining\n",
    "                time_left = max(0, self.collection_time - elapsed_time)\n",
    "                collecting_status = f\"Collecting: {elapsed_time:.1f}s / {self.collection_time:.1f}s\"\n",
    "\n",
    "                # Add a progress bar\n",
    "                progress = int(min(elapsed_time / self.collection_time, 1.0) * 200)\n",
    "                cv2.rectangle(frame, (10, 150), (210, 170), (0, 0, 0), -1)\n",
    "                cv2.rectangle(frame, (10, 150), (10 + progress, 170), (0, 255, 0), -1)\n",
    "\n",
    "                # If collection time is over\n",
    "                if elapsed_time >= self.collection_time:\n",
    "                    self.is_collecting = False\n",
    "\n",
    "                    # Select evenly spaced frames from all collected\n",
    "                    selected_frames = self.select_evenly_spaced_frames()\n",
    "\n",
    "                    # Update frame buffer with selected frames\n",
    "                    self.frame_buffer = deque(selected_frames, maxlen=self.frame_count)\n",
    "\n",
    "                    # Make prediction\n",
    "                    prediction_result, confidence = self.predict_gesture()\n",
    "                    collecting_status = \"Press 'c' to collect new frames\"\n",
    "\n",
    "                    # Save and visualize processed frames\n",
    "                    self.save_processed_frames()\n",
    "                    self.visualize_processed_frames(prediction_result, confidence)\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "            # Key handling\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('c'):\n",
    "                # Start collecting frames\n",
    "                self.is_collecting = True\n",
    "                self.all_processed_frames = []\n",
    "                self.frame_buffer.clear()\n",
    "                self.hand_detected_count = 0\n",
    "                self.collection_start_time = time.time()\n",
    "                collecting_status = \"Collecting started...\"\n",
    "\n",
    "        # Release the capture and close windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with the path to your trained model\n",
    "    model_path = \"hand_gesture_model.pth\"\n",
    "\n",
    "    # Initialize and run the gesture recognition system\n",
    "    # Set collection_time to 2.0 seconds\n",
    "    gesture_recognition = GestureRecognition(model_path, collection_time=1.0)\n",
    "    gesture_recognition.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f82cea-3740-4ea3-8263-5b6e7966d696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# Import from local files\n",
    "from global_defines import DATASET_PATH as data_path\n",
    "#from load_dataset import HandGestureDataset, load_dataset\n",
    "from load_dataset2 import load_dataset\n",
    "from model import Conv3DBlock, DenseBlock3D, TransitionLayer3D, DenseNet3D\n",
    "from training import train_epoch, evaluate, train_and_evaluate\n",
    "from visualize import visualize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202cee85-6bb1-4e27-9790-04654861eda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d9d74c-1c06-4727-86b3-afbbd71d74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq\\\\backend\\\\cython\\\\checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "X, y, subject_ids = load_dataset(data_path)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of sequences: {len(X)}\")\n",
    "print(f\"Number of subjects: {len(np.unique(subject_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a5ae72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "17a5ae72",
    "outputId": "67bf57c5-0e5a-4705-b730-995454d5291f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "hand_gesture_model_path = \"../model/DENSE_NET.model\"\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_loader, valid_loader, test_loader = load_dataset(data_path, [0.7,0.2,0.1], batch_size = 10)\n",
    "\n",
    "# Run with DenseNet model\n",
    "print(\"\\n--- Training with 3D DenseNet ---\")\n",
    "densenet_results, best_model_state = train_and_evaluate(X, y, subject_ids, model_type='densenet')\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\n--- Overall Results ---\")\n",
    "print(f\"DenseNet Overall Accuracy: {densenet_results['overall_accuracy']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "print(\"\\nGenerating visualization plots...\")\n",
    "visualize_results(densenet_results, 'densenet')\n",
    "\n",
    "# Save the model\n",
    "print(\"\\nSaving model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(device)\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), hand_gesture_model_path)\n",
    "print(\"Model saved successfully to 'hand_gesture_model_path'\")\n",
    "\n",
    "print(\"\\nDone! Results saved as PNG files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2739c45a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "2739c45a",
    "outputId": "7b936f5b-405c-4ac9-bf9e-062091f66269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Error loading model: [Errno 2] No such file or directory: 'hand_gesture_model.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\AppData\\Local\\Temp\\ipykernel_2624\\1371429381.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess and load a single sequence\n",
    "def preprocess_sequence(sequence_folder_path):\n",
    "    \"\"\"\n",
    "    Preprocesses a single sequence of images from a folder.\n",
    "\n",
    "    Args:\n",
    "        sequence_folder_path: Path to folder containing sequence frames\n",
    "\n",
    "    Returns:\n",
    "        Tensor with shape [1, C, F, H, W] (batch, channels, frames, height, width)\n",
    "    \"\"\"\n",
    "    # Same transforms as used during training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # Using power of 2 dimensions for better downsampling\n",
    "        transforms.ToTensor(),  # Converts to [0,1] range and changes to CxHxW format\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] range\n",
    "    ])\n",
    "\n",
    "    # Load frames and sort them\n",
    "    frame_files = sorted([f for f in os.listdir(sequence_folder_path)\n",
    "                         if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "    # Process frames\n",
    "    frames = []\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(sequence_folder_path, frame_file)\n",
    "        # Open image and apply transformations\n",
    "        img = Image.open(frame_path).convert('L')  # Convert to grayscale\n",
    "        img_tensor = transform(img)  # Apply transforms\n",
    "        frames.append(img_tensor)\n",
    "\n",
    "    # Pad or truncate sequence to fixed length (8 frames - power of 2)\n",
    "    target_frames = 8\n",
    "    if len(frames) < target_frames:\n",
    "        # Pad with zeros if sequence is too short\n",
    "        for _ in range(target_frames - len(frames)):\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "    elif len(frames) > target_frames:\n",
    "        # Truncate if sequence is too long\n",
    "        frames = frames[:target_frames]\n",
    "\n",
    "    # Stack frames along a new dimension\n",
    "    sequence_tensor = torch.stack(frames)  # Shape: [frames, channels, height, width]\n",
    "    sequence_tensor = sequence_tensor.permute(1, 0, 2, 3)  # Reshape to [channels, frames, height, width]\n",
    "\n",
    "    # Add batch dimension\n",
    "    sequence_tensor = sequence_tensor.unsqueeze(0)  # Shape: [1, channels, frames, height, width]\n",
    "\n",
    "    return sequence_tensor\n",
    "\n",
    "\n",
    "# Function to predict on a single sequence\n",
    "def predict_sequence(model, sequence_tensor, device):\n",
    "    \"\"\"\n",
    "    Makes a prediction on a single sequence.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        sequence_tensor: Preprocessed sequence tensor\n",
    "        device: Device to use for computation\n",
    "\n",
    "    Returns:\n",
    "        predicted_class: Predicted class index\n",
    "        confidence: Confidence score (probability)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence_tensor = sequence_tensor.to(device)\n",
    "        outputs = model(sequence_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "        return predicted.item(), confidence.item()\n",
    "\n",
    "\n",
    "# Function to predict all sequences in a directory\n",
    "def predict_all_sequences(model, sequences_dir, device):\n",
    "    \"\"\"\n",
    "    Predicts classes for all sequences in the given directory.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        sequences_dir: Directory containing sequence folders\n",
    "        device: Device to use for computation\n",
    "\n",
    "    Returns:\n",
    "        results: Dictionary with sequence names as keys and predictions as values\n",
    "    \"\"\"\n",
    "    # Map numerical labels to gesture names\n",
    "    label_map = {0: 'down', 1: 'left', 2: 'right', 3: 'up'}\n",
    "\n",
    "    results = {}\n",
    "    sequence_folders = sorted([f for f in os.listdir(sequences_dir)\n",
    "                              if os.path.isdir(os.path.join(sequences_dir, f))])\n",
    "\n",
    "    for sequence_folder in sequence_folders:\n",
    "        sequence_path = os.path.join(sequences_dir, sequence_folder)\n",
    "\n",
    "        # Preprocess the sequence\n",
    "        sequence_tensor = preprocess_sequence(sequence_path)\n",
    "\n",
    "        # Make prediction\n",
    "        predicted_class, confidence = predict_sequence(model, sequence_tensor, device)\n",
    "\n",
    "        # Store result\n",
    "        results[sequence_folder] = {\n",
    "            'predicted_class': predicted_class,\n",
    "            'gesture_name': label_map[predicted_class],\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "        print(f\"Sequence: {sequence_folder}, Predicted: {label_map[predicted_class]}, Confidence: {confidence:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_predictions(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes prediction results.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary with prediction results\n",
    "        save_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    sequences = list(results.keys())\n",
    "    confidences = [results[seq]['confidence'] for seq in sequences]\n",
    "    gestures = [results[seq]['gesture_name'] for seq in sequences]\n",
    "\n",
    "    # Set colors based on gesture\n",
    "    colors = {'down': 'blue', 'left': 'green', 'right': 'red', 'up': 'purple'}\n",
    "    bar_colors = [colors[gesture] for gesture in gestures]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(sequences, confidences, color=bar_colors)\n",
    "\n",
    "    # Add gesture labels on top of bars\n",
    "    for bar, gesture in zip(bars, gestures):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 gesture, ha='center', fontsize=9)\n",
    "\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xlabel('Sequence')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title('Hand Gesture Predictions')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Create legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color, label=gesture)\n",
    "                      for gesture, color in colors.items()]\n",
    "    plt.legend(handles=legend_elements)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main function to load model and predict\n",
    "def main():\n",
    "    # Path to the saved model\n",
    "    model_path = 'hand_gesture_model.pth'\n",
    "\n",
    "    # Path to the directory containing sample sequences\n",
    "    sequences_dir = 'D/sample_sequence'\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize the model with the same architecture\n",
    "    #model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(device)\n",
    "    model = DenseNet3D(block_config=(2, 4, 4), num__init__features=16).to(device)\n",
    "\n",
    "    # Load the trained model weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Predict all sequences\n",
    "    print(\"\\nMaking predictions on sample sequences...\")\n",
    "    results = predict_all_sequences(model, sequences_dir, device)\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"\\nVisualizing predictions...\")\n",
    "    visualize_predictions(results, save_path='predictions_visualization.png')\n",
    "\n",
    "    # Save results to file\n",
    "    import json\n",
    "    with open('prediction_results.json', 'w') as f:\n",
    "        # Convert results to serializable format\n",
    "        serializable_results = {}\n",
    "        for key, value in results.items():\n",
    "            serializable_results[key] = {\n",
    "                'predicted_class': int(value['predicted_class']),\n",
    "                'gesture_name': value['gesture_name'],\n",
    "                'confidence': float(value['confidence'])\n",
    "            }\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "\n",
    "    print(\"Results saved to prediction_results.json\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a12481",
   "metadata": {
    "id": "f2a12481"
   },
   "outputs": [],
   "source": [
    "\"\"\"Best Live Prediction UpTill Now\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from collections import deque\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Recreate the DenseNet3D model to match your training architecture\n",
    "class Conv3DBlock(nn.Module):\n",
    "    def ___init___(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv3DBlock, self).___init___()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class DenseBlock3D(nn.Module):\n",
    "    def ___init___(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock3D, self).___init___()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(self._make_dense_layer(in_channels + i * growth_rate, growth_rate))\n",
    "\n",
    "    def _make_dense_layer(self, in_channels, growth_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, 4 * growth_rate, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm3d(4 * growth_rate),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            new_feature = layer(torch.cat(features, 1))\n",
    "            features.append(new_feature)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class TransitionLayer3D(nn.Module):\n",
    "    def ___init___(self, in_channels, out_channels):\n",
    "        super(TransitionLayer3D, self).___init___()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DenseNet3D(nn.Module):\n",
    "    def ___init___(self, growth_rate=12, block_config=(2, 4, 4), num__init__features=16,\n",
    "                 compression_rate=0.5, num_classes=4):\n",
    "        super(DenseNet3D, self).___init___()\n",
    "\n",
    "        # First convolution and pooling\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, num__init__features, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(num__init__features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Dense Blocks\n",
    "        num_features = num__init__features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Add a dense block\n",
    "            block = DenseBlock3D(\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                num_layers=num_layers\n",
    "            )\n",
    "            self.features.add_module(f'denseblock{i+1}', block)\n",
    "            num_features += num_layers * growth_rate\n",
    "\n",
    "            # Add a transition layer (except after the last block)\n",
    "            if i != len(block_config) - 1:\n",
    "                out_features = int(num_features * compression_rate)\n",
    "                trans = TransitionLayer3D(num_features, out_features)\n",
    "                self.features.add_module(f'transition{i+1}', trans)\n",
    "                num_features = out_features\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm_final', nn.BatchNorm3d(num_features))\n",
    "        self.features.add_module('relu_final', nn.ReLU(inplace=True))\n",
    "\n",
    "        # Global Average Pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.avgpool(features)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GestureRecognition:\n",
    "    def ___init___(self, model_path, frame_count=8, collection_time=2.0, display_fps=True):\n",
    "        # Initialize MediaPipe Hands\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "        # Load the trained model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num__init__features=16).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "\n",
    "        # Gesture classes\n",
    "        self.gesture_classes = ['down', 'left', 'right', 'up']\n",
    "\n",
    "        # Initialize variables for frames processing\n",
    "        self.frame_count = frame_count\n",
    "        self.collection_time = collection_time  # Time in seconds to collect frames\n",
    "        self.frame_buffer = deque(maxlen=frame_count)\n",
    "        self.all_processed_frames = []  # Store all processed frames during collection\n",
    "        self.is_collecting = False\n",
    "        self.hand_detected_count = 0\n",
    "        self.min_hand_detected = 6  # Minimum number of frames that must have a hand\n",
    "\n",
    "        # Collection timing variables\n",
    "        self.collection_start_time = 0\n",
    "        self.collection_end_time = 0\n",
    "\n",
    "        # Define the same transformation pipeline as in training\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "        # FPS calculation\n",
    "        self.prev_frame_time = 0\n",
    "        self.new_frame_time = 0\n",
    "        self.display_fps = display_fps\n",
    "\n",
    "        # Create directory for saving processed frames\n",
    "        self.frames_dir = \"processed_frames\"\n",
    "        os.makedirs(self.frames_dir, exist_ok=True)\n",
    "\n",
    "    def preprocess_hand_frame(self, frame, results):\n",
    "        \"\"\"Extract hand from frame and preprocess it\"\"\"\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # If no hand is detected, return None\n",
    "        if not results.multi_hand_landmarks:\n",
    "            return None, None\n",
    "\n",
    "        # Get landmarks for the first hand detected\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Calculate bounding box with padding\n",
    "        x_min, x_max, y_min, y_max = w, 0, h, 0\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "            x_min = min(x_min, x)\n",
    "            x_max = max(x_max, x)\n",
    "            y_min = min(y_min, y)\n",
    "            y_max = max(y_max, y)\n",
    "\n",
    "        # Add padding\n",
    "        padding = 20\n",
    "        x_min = max(0, x_min - padding)\n",
    "        y_min = max(0, y_min - padding)\n",
    "        x_max = min(w, x_max + padding)\n",
    "        y_max = min(h, y_max + padding)\n",
    "\n",
    "        # Check if bounding box is valid\n",
    "        if x_min >= x_max or y_min >= y_max:\n",
    "            return None, None\n",
    "\n",
    "        # Crop the hand region\n",
    "        hand_crop = frame[y_min:y_max, x_min:x_max].copy()\n",
    "\n",
    "        # Create a copy of the original frame with hand landmarks\n",
    "        marked_frame = frame.copy()\n",
    "        self.mp_drawing.draw_landmarks(\n",
    "            marked_frame,\n",
    "            hand_landmarks,\n",
    "            self.mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(marked_frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "        # Convert to grayscale and PIL image\n",
    "        hand_crop_gray = cv2.cvtColor(hand_crop, cv2.COLOR_BGR2GRAY)\n",
    "        pil_img = Image.fromarray(hand_crop_gray)\n",
    "\n",
    "        # Apply transforms\n",
    "        img_tensor = self.transform(pil_img)\n",
    "\n",
    "        return img_tensor, marked_frame\n",
    "\n",
    "    def predict_gesture(self):\n",
    "        \"\"\"Make a prediction based on collected frames\"\"\"\n",
    "        if len(self.frame_buffer) < self.frame_count:\n",
    "            return \"Insufficient frames\", 0.0\n",
    "\n",
    "        if self.hand_detected_count < self.min_hand_detected:\n",
    "            return \"Hand not consistently detected\", 0.0\n",
    "\n",
    "        # Stack frames into a sequence\n",
    "        sequence = torch.stack(list(self.frame_buffer))  # Shape: [frames, channels, height, width]\n",
    "        sequence = sequence.permute(1, 0, 2, 3).unsqueeze(0)  # Reshape to [1, channels, frames, height, width]\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            sequence = sequence.to(self.device)\n",
    "            outputs = self.model(sequence)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            confidence, prediction = torch.max(probabilities, 1)\n",
    "\n",
    "        # Get prediction class and confidence\n",
    "        predicted_class = self.gesture_classes[prediction.item()]\n",
    "        confidence_value = confidence.item()\n",
    "\n",
    "        return predicted_class, confidence_value\n",
    "\n",
    "    def save_processed_frames(self):\n",
    "        \"\"\"Save the processed frames for visualization\"\"\"\n",
    "        # Clear previous frames\n",
    "        for file in os.listdir(self.frames_dir):\n",
    "            os.remove(os.path.join(self.frames_dir, file))\n",
    "\n",
    "        # Save current frame buffer\n",
    "        for i, frame_tensor in enumerate(self.frame_buffer):\n",
    "            # Convert tensor to numpy array for visualization\n",
    "            frame_np = frame_tensor.cpu().numpy()[0]  # Get the first channel\n",
    "            frame_np = (frame_np * 0.5 + 0.5) * 255  # Denormalize\n",
    "            frame_np = frame_np.astype(np.uint8)\n",
    "\n",
    "            # Save the frame\n",
    "            cv2.imwrite(os.path.join(self.frames_dir, f\"frame_{i+1}.jpg\"), frame_np)\n",
    "\n",
    "    def visualize_processed_frames(self, prediction, confidence):\n",
    "        \"\"\"Plot frames used for prediction\"\"\"\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        for i, frame_path in enumerate(sorted(os.listdir(self.frames_dir))):\n",
    "            plt.subplot(2, 4, i+1)\n",
    "            frame = cv2.imread(os.path.join(self.frames_dir, frame_path), cv2.IMREAD_GRAYSCALE)\n",
    "            plt.imshow(frame, cmap='gray')\n",
    "            plt.title(f\"Frame {i+1}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Prediction: {prediction.upper()} (Confidence: {confidence:.2f})\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"prediction_frames.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def select_evenly_spaced_frames(self):\n",
    "        \"\"\"Select evenly spaced frames from all collected frames\"\"\"\n",
    "        total_frames = len(self.all_processed_frames)\n",
    "        if total_frames <= self.frame_count:\n",
    "            return self.all_processed_frames\n",
    "\n",
    "        # Calculate indices for evenly spaced frames\n",
    "        indices = np.linspace(0, total_frames - 1, self.frame_count, dtype=int)\n",
    "        selected_frames = [self.all_processed_frames[i] for i in indices]\n",
    "        return selected_frames\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the gesture recognition system\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        # Status variables\n",
    "        collecting_status = \"Press 'c' to start collecting frames\"\n",
    "        prediction_result = \"No prediction yet\"\n",
    "        confidence = 0.0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Calculate FPS\n",
    "            if self.display_fps:\n",
    "                self.new_frame_time = time.time()\n",
    "                fps = 1/(self.new_frame_time - self.prev_frame_time) if self.prev_frame_time > 0 else 0\n",
    "                self.prev_frame_time = self.new_frame_time\n",
    "                cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = self.hands.process(rgb_frame)\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Display collection status\n",
    "            cv2.putText(frame, collecting_status, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Display prediction result\n",
    "            cv2.putText(frame, f\"Prediction: {prediction_result}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if confidence > 0:\n",
    "                cv2.putText(frame, f\"Confidence: {confidence:.2f}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # If collecting frames\n",
    "            if self.is_collecting:\n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - self.collection_start_time\n",
    "\n",
    "                # Process the current frame\n",
    "                processed_frame, marked_frame = self.preprocess_hand_frame(frame, results)\n",
    "\n",
    "                if processed_frame is not None:\n",
    "                    self.all_processed_frames.append(processed_frame)\n",
    "                    self.hand_detected_count += 1\n",
    "                    if marked_frame is not None:\n",
    "                        frame = marked_frame\n",
    "\n",
    "                # Update collection status with time remaining\n",
    "                time_left = max(0, self.collection_time - elapsed_time)\n",
    "                collecting_status = f\"Collecting: {elapsed_time:.1f}s / {self.collection_time:.1f}s\"\n",
    "\n",
    "                # Add a progress bar\n",
    "                progress = int(min(elapsed_time / self.collection_time, 1.0) * 200)\n",
    "                cv2.rectangle(frame, (10, 150), (210, 170), (0, 0, 0), -1)\n",
    "                cv2.rectangle(frame, (10, 150), (10 + progress, 170), (0, 255, 0), -1)\n",
    "\n",
    "                # If collection time is over\n",
    "                if elapsed_time >= self.collection_time:\n",
    "                    self.is_collecting = False\n",
    "\n",
    "                    # Select evenly spaced frames from all collected\n",
    "                    selected_frames = self.select_evenly_spaced_frames()\n",
    "\n",
    "                    # Update frame buffer with selected frames\n",
    "                    self.frame_buffer = deque(selected_frames, maxlen=self.frame_count)\n",
    "\n",
    "                    # Make prediction\n",
    "                    prediction_result, confidence = self.predict_gesture()\n",
    "                    collecting_status = \"Press 'c' to collect new frames\"\n",
    "\n",
    "                    # Save and visualize processed frames\n",
    "                    self.save_processed_frames()\n",
    "                    self.visualize_processed_frames(prediction_result, confidence)\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "            # Key handling\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('c'):\n",
    "                # Start collecting frames\n",
    "                self.is_collecting = True\n",
    "                self.all_processed_frames = []\n",
    "                self.frame_buffer.clear()\n",
    "                self.hand_detected_count = 0\n",
    "                self.collection_start_time = time.time()\n",
    "                collecting_status = \"Collecting started...\"\n",
    "\n",
    "        # Release the capture and close windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with the path to your trained model\n",
    "    model_path = \"hand_gesture_model.pth\"\n",
    "\n",
    "    # Initialize and run the gesture recognition system\n",
    "    # Set collection_time to 2.0 seconds\n",
    "    gesture_recognition = GestureRecognition(model_path, collection_time=1.0)\n",
    "    gesture_recognition.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

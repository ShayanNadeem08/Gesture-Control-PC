{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf4d36-4338-4911-a08e-c5f08df798e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IN THE NAME OF  ALLAH , THE MOST GRACIOUS, THE MOST MERCIFUL. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25326091-165a-4f57-8abf-7070d8bb0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "modules_path = '../external_modules/'\n",
    "dataset_path = '../../Dataset/mp'\n",
    "save_path = \"../model\"\n",
    "\n",
    "sys.path.insert(1, modules_path)\n",
    "from video_dataset import VideoFrameDataset, ImglistToTensor\n",
    "from global_defines import GESTURE_MAP_NUM2STR\n",
    "\n",
    "if False and torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f832a2f-7ddf-471d-b04b-3b7233438c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "#img_w = 640\n",
    "#img_h = 480\n",
    "#frames_per_video = 7\n",
    "batch_size = 8\n",
    "num_workers = 1\n",
    "\n",
    "dataset = VideoFrameDataset(\n",
    "    root_path= f\"{dataset_path}\",\n",
    "    annotationfile_path=f\"{dataset_path}/annotations.txt\",\n",
    "    num_segments=8,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='{:01d}.jpg',\n",
    "    transform=ImglistToTensor(),\n",
    "    test_mode=False\n",
    ")\n",
    "class_map = {0:\"down\", 1:\"left\", 2:\"right\", 3:\"up\"}\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [0.7,0.2,0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f642bad4-d373-4a2f-945b-4bd9ce5b6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessings\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.35,\n",
    "    min_tracking_confidence=0.35\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def get_hand_landmarks_frame(frame, istransform=False):\n",
    "    if istransform==True:\n",
    "        frame = 255*frame\n",
    "        frame = frame.transpose(2,0).numpy().astype('uint8')\n",
    "    results = hands.process(frame)\n",
    "\n",
    "    hand_landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            hand_landmarks.append([hand_landmark.x, hand_landmark.y])\n",
    "        hand_landmarks = np.array(hand_landmarks).flatten()\n",
    "    else:\n",
    "        hand_landmarks = np.zeros(21*2)\n",
    "\n",
    "    return torch.from_numpy(hand_landmarks.astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33486d79-c9b6-4082-af45-4e3fd03bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NN_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NN_LSTM, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(layer_sizes[0],layer_sizes[1])\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.lstm = torch.nn.LSTM(layer_sizes[1],layer_sizes[2],batch_first=True)\n",
    "        self.linear2 = torch.nn.Linear(layer_sizes[2],layer_sizes[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.linear1(x)\n",
    "        out2 = self.relu1(out1)\n",
    "        out3 = self.lstm(out2)\n",
    "        out4 = self.linear2(out3[0])\n",
    "        return out4\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "model = NN_LSTM([21*2,10,5,5]).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2459db1b-6d08-43d1-b6f3-241ff56cd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train(model, data):\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        print(\"\\r\"+str(i), end=\"\")\n",
    "            \n",
    "        y = y.to(device)\n",
    "        hand_landmarks = torch.zeros(batch_size, 8, 21*2)\n",
    "\n",
    "        for i, video in enumerate(x):    # x is batch\n",
    "            for j, frame in enumerate(video):\n",
    "                hand_landmarks[i][j] = get_hand_landmarks_frame(frame, True)\n",
    "        \n",
    "        y_hat = model(hand_landmarks)\n",
    "        y_hat = y_hat.transpose(0,1)[-1]\n",
    "        loss = loss_function(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, data, label=\"\"):\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k, (x,y) in enumerate(data):\n",
    "            hand_landmarks = torch.zeros(batch_size, 8, 21*2)\n",
    "\n",
    "            for i, video in enumerate(x):    # x is batch\n",
    "                for j, frame in enumerate(video):\n",
    "                    hand_landmarks[i][j] = get_hand_landmarks_frame(frame, True)\n",
    "\n",
    "            y_hat = model(hand_landmarks)\n",
    "            y_hat = y_hat.transpose(0,1)[-1]\n",
    "            _, y_hat = torch.max(y_hat,1)\n",
    "            correct += (y_hat==y).sum()\n",
    "    print(label+\"accuracy:\", round(float(correct/(k+1)/batch_size), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c9c0ded-4d9a-4614-bbb0-81950d59b0e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data)\u001b[0m\n\u001b[0;32m     14\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model(hand_landmarks)\n\u001b[0;32m     15\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_hat, y)\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         target,\n\u001b[0;32m   1296\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1297\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1298\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1299\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1300\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3481\u001b[0m     target,\n\u001b[0;32m   3482\u001b[0m     weight,\n\u001b[0;32m   3483\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3484\u001b[0m     ignore_index,\n\u001b[0;32m   3485\u001b[0m     label_smoothing,\n\u001b[0;32m   3486\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (2)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train\n",
    "NumEpochs=1 \n",
    "for epoch in range(NumEpochs):\n",
    "    loss = train(model, train_loader)\n",
    "    print(\"  Epoch:\",epoch, \" Loss:\", round(loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadea647-0243-409d-bab1-807155fa4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, save_path+\"/model_lstm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bdb60-7ed6-4146-81d7-338bd02f9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(save_path+\"/model_lstm.model\", map_location=torch.device('cpu')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aac234-dcf3-4c76-8e94-7057d865f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Test\n",
    "test(model, train_loader, \"Training\")\n",
    "test(model, valid_loader, \"Vailidation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0563009-2392-4ddb-a487-63def436e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, label=\"\"):\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k, x in enumerate(data):\n",
    "            hand_landmarks = torch.zeros(batch_size, 8, 63)\n",
    "\n",
    "            for i, video in enumerate(x):    # x is batch\n",
    "                for j, frame in enumerate(video):\n",
    "                    hand_landmarks[i][j] = get_hand_landmarks_frame(frame)\n",
    "\n",
    "            y_hat = model(hand_landmarks)\n",
    "            y_hat = y_hat.transpose(0,1)[-1]\n",
    "            confidence, y_hat = torch.max(y_hat,1)\n",
    "            print(y_hat)\n",
    "            if confidence[0].item() >= 0.6:\n",
    "                print(GESTURE_MAP_NUM2STR[y_hat[0].item()], confidence[0].item())\n",
    "            else:\n",
    "                print(\"None\", confidence[0].item())\n",
    "\n",
    "def LiveVideoTest():\n",
    "    # Captures videos\n",
    "    num_batches = 10\n",
    "    batch_size = 1\n",
    "    frames_per_video = 8\n",
    "    frame_rate = 10\n",
    "\n",
    "    for n in range(num_batches):\n",
    "        batch = []\n",
    "        for v in range(batch_size):\n",
    "            video = []\n",
    "            for f in range(frames_per_video):\n",
    "                ret, frame = cam.read()\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                video.append(frame)\n",
    "\n",
    "                # Draw hand landmarks on the frame\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        self.mp_drawing.draw_landmarks(\n",
    "                            frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n",
    "                cv2.imshow('Hand Gesture Recognition', frame)\n",
    "                    \n",
    "                time.sleep(1/frame_rate)\n",
    "                \n",
    "            batch.append(video)\n",
    "        predict(model,[batch])\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "try:\n",
    "    LiveVideoTest()\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d4ea2-4b3d-43ba-9851-b8aa4a3e6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "LiveVideoTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed8c66-ccd1-49a8-aaeb-f26754102100",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce37ec-f872-4f94-810d-627abfaf4958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

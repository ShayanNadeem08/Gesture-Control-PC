{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affb8ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([1840, 1, 8, 64, 64])\n",
      "Number of sequences: 1840\n",
      "Number of subjects: 460\n",
      "\n",
      "--- Training with 3D DenseNet ---\n",
      "Training started\n",
      "Using device: cpu\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Epoch 1/25: Train Loss: 1.3268, Train Acc: 38.52%, Test Loss: 1.2769, Test Acc: 40.49%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 521\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Run with DenseNet model\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training with 3D DenseNet ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 521\u001b[0m densenet_results, best_model_state \u001b[38;5;241m=\u001b[39m train_and_evaluate(X, y, subject_ids, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensenet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# Print overall results\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Overall Results ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 408\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(X, y, subject_ids, model_type)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer, device)\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     test_loss, test_acc, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[1], line 288\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    285\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    289\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import LeaveOneGroupOut, KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Custom Dataset class for hand gestures\n",
    "class HandGestureDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Define Mediapipe processing\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.25,\n",
    "    min_tracking_confidence=0.25\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def get_hand_landmarks_frame(frame, is_transform=False):\n",
    "    if is_transform==True:\n",
    "        frame = 255*frame\n",
    "        frame = frame.transpose(2,0).numpy().astype('uint8')\n",
    "    results = hands.process(frame)\n",
    "\n",
    "    hand_landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            hand_landmarks.append([hand_landmark.x, hand_landmark.y])\n",
    "        hand_landmarks = np.array(hand_landmarks)\n",
    "    else:\n",
    "        hand_landmarks = np.zeros((21,2))\n",
    "\n",
    "    return torch.from_numpy(hand_landmarks.astype('float32'))\n",
    "\n",
    "def get_trace_frame(frame, is_transform=False):\n",
    "    trace_frame = torch.zeros(64,64)\n",
    "    hand_landmarks = get_hand_landmarks_frame(frame, is_transform)\n",
    "    for (x,y) in hand_landmarks:\n",
    "        x = int(63*x)\n",
    "        y = int(63*y)\n",
    "        \n",
    "        N = 63\n",
    "        trace_frame[min(x,N)][min(y,N)] = 1\n",
    "        \n",
    "        trace_frame[min(x+1,N)][min(y,N)] = 1\n",
    "        trace_frame[min(max(x-1,0),N)][min(y,N)] = 1\n",
    "        trace_frame[min(x,N)][min(y+1,N)] = 1\n",
    "        trace_frame[min(x,N)][min(max(y-1,0),N)] = 1\n",
    "        \n",
    "        trace_frame[min(x+1,N)][min(y+1,N)] = 1\n",
    "        trace_frame[min(max(x-1,0),N)][min(y+1,N)] = 1\n",
    "        trace_frame[min(x+1,N)][min(max(y-1,0),N)] = 1\n",
    "        trace_frame[min(max(x-1,0),N)][min(max(y-1,0),N)] = 1\n",
    "    return trace_frame\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_dataset(data_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the hand gesture dataset with background blurring.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the dataset directory\n",
    "        \n",
    "    Returns:\n",
    "        X: Sequences of images with shape (num_sequences, channels, frames, height, width)\n",
    "        y: Labels corresponding to gestures (0: down, 1: left, 2: right, 3: up)\n",
    "        subject_ids: IDs of subjects for cross-validation\n",
    "    \"\"\"\n",
    "    # Placeholder arrays for data, labels, and subject IDs\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    subject_ids = []\n",
    "    \n",
    "    # Map gesture names to numerical labels\n",
    "    gesture_map = {'down': 0, 'left': 1, 'right': 2, 'up': 3}\n",
    "    \n",
    "    # Image transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # Using power of 2 dimensions for better downsampling\n",
    "        transforms.ToTensor(),  # Converts to [0,1] range and changes to CxHxW format\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1] range\n",
    "        #transforms.ColorJitter(brightness=0.5, contrast=1, saturation=0.1, hue=0.5)\n",
    "    ])\n",
    "    \n",
    "    # Iterate through gesture folders\n",
    "    for gesture in ['down', 'left', 'right', 'up']:\n",
    "        gesture_path = os.path.join(data_path, gesture)\n",
    "        \n",
    "        # Iterate through sequence folders for this gesture\n",
    "        for seq_folder in os.listdir(gesture_path):\n",
    "            seq_path = os.path.join(gesture_path, seq_folder)\n",
    "            \n",
    "            if os.path.isdir(seq_path):\n",
    "                # Extract subject ID from sequence folder name (assuming format contains subject ID)\n",
    "                # Modify this according to your actual folder naming convention\n",
    "                subject_id = int(seq_folder.split('_')[0])\n",
    "                \n",
    "                # Load frames for this sequence\n",
    "                frame_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "                \n",
    "                if len(frame_files) > 0:\n",
    "                    # Load and normalize frames\n",
    "                    frames = []\n",
    "                    for frame_file in frame_files:\n",
    "                        frame_path = os.path.join(seq_path, frame_file)\n",
    "                        \n",
    "                        # Open image with OpenCV for skin detection and background blurring\n",
    "                        img_cv = cv2.imread(frame_path)\n",
    "                        if img_cv is not None:\n",
    "                            img_tensor = get_trace_frame(img_cv)\n",
    "                            frames.append(img_tensor)\n",
    "                    \n",
    "                    # Pad or truncate sequence to fixed length (8 frames - power of 2)\n",
    "                    target_frames = 8\n",
    "                    if len(frames) < target_frames:\n",
    "                        # Pad with zeros if sequence is too short\n",
    "                        for _ in range(target_frames - len(frames)):\n",
    "                            frames.append(torch.zeros_like(frames[0]))\n",
    "                    elif len(frames) > target_frames:\n",
    "                        # Truncate if sequence is too long\n",
    "                        frames = frames[:target_frames]\n",
    "                    \n",
    "                    # Stack frames along a new dimension\n",
    "                    sequence_tensor = torch.from_numpy(np.array([frames]))   # Reshape to [channels, frames, height, width]\n",
    "\n",
    "                    # Add sequence to dataset\n",
    "                    sequences.append(sequence_tensor)\n",
    "                    labels.append(gesture_map[gesture])\n",
    "                    subject_ids.append(subject_id)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X = torch.stack(sequences)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    subject_ids = np.array(subject_ids)\n",
    "    \n",
    "    return X, y, subject_ids\n",
    "\n",
    "# Dense Block for 3D DenseNet\n",
    "class DenseBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock3D, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(self._make_dense_layer(in_channels + i * growth_rate, growth_rate))\n",
    "    \n",
    "    def _make_dense_layer(self, in_channels, growth_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, 4 * growth_rate, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm3d(4 * growth_rate),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            new_feature = layer(torch.cat(features, 1))\n",
    "            features.append(new_feature)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "# Transition Layer for 3D DenseNet\n",
    "class TransitionLayer3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionLayer3D, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# 3D DenseNet Model\n",
    "class DenseNet3D(nn.Module):\n",
    "    def __init__(self, growth_rate=12, block_config=(2, 4, 6), num_init_features=16, \n",
    "                 compression_rate=0.5, num_classes=4):\n",
    "        super(DenseNet3D, self).__init__()\n",
    "        \n",
    "        # First convolution and pooling\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, num_init_features, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Dense Blocks\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Add a dense block\n",
    "            block = DenseBlock3D(\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                num_layers=num_layers\n",
    "            )\n",
    "            self.features.add_module(f'denseblock{i+1}', block)\n",
    "            num_features += num_layers * growth_rate\n",
    "            \n",
    "            # Add a transition layer (except after the last block)\n",
    "            if i != len(block_config) - 1:\n",
    "                out_features = int(num_features * compression_rate)\n",
    "                trans = TransitionLayer3D(num_features, out_features)\n",
    "                self.features.add_module(f'transition{i+1}', trans)\n",
    "                num_features = out_features\n",
    "        \n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm_final', nn.BatchNorm3d(num_features))\n",
    "        self.features.add_module('relu_final', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Global Average Pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.lstm = nn.LSTM(num_features,num_features)\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.avgpool(features)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.lstm(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to train the model for one epoch\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Save predictions and targets for confusion matrix\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    test_loss = running_loss / total\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc, all_preds, all_targets\n",
    "\n",
    "\n",
    "def train_and_evaluate(X, y, subject_ids, model_type='densenet'):\n",
    "    print(\"Training started\")\n",
    "    \"\"\"\n",
    "    Train and evaluate the 3D-CNN model using k-fold cross-validation\n",
    "    with stratification across subjects.\n",
    "    \n",
    "    Args:\n",
    "        X: Sequences of images tensor\n",
    "        y: Labels tensor\n",
    "        subject_ids: Subject IDs for cross-validation\n",
    "        model_type: Type of model to use\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionary containing evaluation results\n",
    "        best_model_state: State dict of the best performing model\n",
    "    \"\"\"\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize KFold cross-validator\n",
    "    n_splits = 5  # 5-fold cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    subject_accuracies = []\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    # Cross-validation parameters\n",
    "    num_epochs = 25\n",
    "    batch_size = 16\n",
    "    \n",
    "    # Track the best model across all folds\n",
    "    best_overall_acc = 0\n",
    "    final_best_model_state = None\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X.numpy()), 1):\n",
    "        print(f\"\\n--- Fold {fold}/{n_splits} ---\")\n",
    "        \n",
    "        # Create PyTorch datasets and dataloaders\n",
    "        train_dataset = HandGestureDataset(X[train_idx], y[train_idx])\n",
    "        test_dataset = HandGestureDataset(X[test_idx], y[test_idx])\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Create the model\n",
    "        model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num_init_features=16).to(device)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=0.01,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=num_epochs\n",
    "        )\n",
    "        \n",
    "        # Variables for early stopping\n",
    "        best_test_acc = 0\n",
    "        best_model_state = None\n",
    "        patience = 15\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            # Train for one epoch\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "            \n",
    "            # Check for improvement\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model for final evaluation\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Final evaluation\n",
    "        _, final_test_acc, y_pred, y_true = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Store results\n",
    "        subject_accuracies.append(final_test_acc / 100.0)\n",
    "        all_y_true.extend(y_true)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        \n",
    "        print(f\"Fold {fold} Final Test Accuracy: {final_test_acc:.2f}%\")\n",
    "        \n",
    "        # Keep track of the best model across all folds\n",
    "        if final_test_acc > best_overall_acc:\n",
    "            best_overall_acc = final_test_acc\n",
    "            final_best_model_state = best_model_state.copy()\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_accuracy = np.mean(subject_accuracies)\n",
    "    conf_matrix = confusion_matrix(all_y_true, all_y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'subject_accuracies': subject_accuracies,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'y_true': all_y_true,\n",
    "        'y_pred': all_y_pred\n",
    "    }\n",
    "    \n",
    "    return results, final_best_model_state\n",
    "\n",
    "\n",
    "# Function to visualize results\n",
    "def visualize_results(results, model_name='densenet'):\n",
    "    \"\"\"\n",
    "    Visualize the cross-validation results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing evaluation results\n",
    "        model_name: Name of the model\n",
    "    \"\"\"\n",
    "    # Plot subject accuracies\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(results['subject_accuracies'])), results['subject_accuracies'])\n",
    "    plt.axhline(y=results['overall_accuracy'], color='r', linestyle='-', \n",
    "                label=f\"Overall: {results['overall_accuracy']:.4f}\")\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Leave-One-Subject-Out Cross-Validation Results ({model_name})')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    plt.savefig(f'subject_accuracies_{model_name}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    gesture_names = ['Down', 'Left', 'Right', 'Up']\n",
    "    conf_matrix = results['confusion_matrix']\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(conf_matrix_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=gesture_names, yticklabels=gesture_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix ({model_name})')\n",
    "    plt.savefig(f'confusion_matrix_{model_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Path to your dataset\n",
    "    data_path = \"../../Dataset/STMM/image\"\n",
    "    \n",
    "    # Load and preprocess the dataset\n",
    "    X, y, subject_ids = load_dataset(data_path)\n",
    "    \n",
    "    # Print dataset info\n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Number of sequences: {len(X)}\")\n",
    "    print(f\"Number of subjects: {len(np.unique(subject_ids))}\")\n",
    "    \n",
    "    # Run with DenseNet model\n",
    "    print(\"\\n--- Training with 3D DenseNet ---\")\n",
    "    densenet_results, best_model_state = train_and_evaluate(X, y, subject_ids, model_type='densenet')\n",
    "    \n",
    "    # Print overall results\n",
    "    print(\"\\n--- Overall Results ---\")\n",
    "    print(f\"DenseNet Overall Accuracy: {densenet_results['overall_accuracy']:.4f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"\\nGenerating visualization plots...\")\n",
    "    visualize_results(densenet_results, 'densenet')\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"\\nSaving model...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DenseNet3D(growth_rate=12, block_config=(2, 4, 4), num_init_features=16).to(device)\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save the model state dictionary\n",
    "    torch.save(model.state_dict(), '../model/densenet_mediapipe_lstm.pth')\n",
    "    print(\"Model saved successfully to 'densenet_mediapipe.pth'\")\n",
    "    \n",
    "    print(\"\\nDone! Results saved as PNG files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596cb96-6e03-40d7-994f-039473e1d0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03343d9-a6c1-4f3b-a15e-677b6dd08bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IN THE NAME OF  ALLAH , THE MOST GRACIOUS, THE MOST MERCIFUL. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b4bf1b-1d63-4213-8de5-512e4fa2a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "modules_path = 'C:/Users/PMLS/FYP/Modules/'\n",
    "dataset_path = 'C:/Users/PMLS/FYP/Dataset/STMM'\n",
    "save_path = \"C:/Users/PMLS/FYP/Models\"\n",
    "\n",
    "sys.path.insert(1, modules_path)\n",
    "from video_dataset import VideoFrameDataset, ImglistToTensor\n",
    "\n",
    "if False and torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d71cba9-db1d-4128-bb44-ec0fa0ff801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "#img_w = 640\n",
    "#img_h = 480\n",
    "#frames_per_video = 7\n",
    "batch_size = 10\n",
    "\n",
    "dataset = VideoFrameDataset(\n",
    "    root_path= f\"{dataset_path}\",\n",
    "    annotationfile_path=f\"{dataset_path}/annotations.txt\",\n",
    "    num_segments=8,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='{:01d}.jpg',\n",
    "    transform=ImglistToTensor(),\n",
    "    test_mode=False\n",
    ")\n",
    "class_map = {0:\"down\", 1:\"left\", 2:\"right\", 3:\"up\"}\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [0.7,0.2,0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c09c63d-e60a-4f6a-8b93-86484e634017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        #self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, 300))\n",
    "        self.lstm = nn.LSTM(input_size=1000, hidden_size=256, num_layers=3)\n",
    "        self.linear1 = nn.Linear(256, 128)\n",
    "        self.linear2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.resnet(x)\n",
    "        out2 = self.lstm(out1)[0]\n",
    "        out3 = self.linear1(out2)\n",
    "        out4 = self.linear2(out3)\n",
    "        return out4 \n",
    "\n",
    "model = CNN_LSTM()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9231b8c-ffe8-4661-90ff-bee58980aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "076aa875-2cf3-40ab-ade6-bdf74c313e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "import cv2\n",
    "import pywt\n",
    "def wavelet_transform(batch):\n",
    "        batch2=[]\n",
    "        for frame in batch:\n",
    "            A, B = pywt.dwt(frame, 'db1')\n",
    "            batch2.append(B)\n",
    "        return np.array(batch2)\n",
    "\n",
    "def grey_transform(batch):\n",
    "    batch = batch.transpose(0,1,3,4,2)\n",
    "    favg = np.average(batch[0], axis=0)\n",
    "    grey_avg = cv2.cvtColor(favg, cv2.COLOR_BGR2GRAY)\n",
    "    batch2 = []\n",
    "    for frame in batch[0]:\n",
    "        frame2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame2 = frame2 - grey_avg\n",
    "        threshhold = np.max(frame2) * 0.05\n",
    "        _, frame2 = cv2.threshold(frame2, threshhold, 255, cv2.THRESH_BINARY)\n",
    "        batch2.append(frame2)\n",
    "    return np.array([batch2])\n",
    "    \n",
    "def local_transform(batch):\n",
    "    batch = np.array(batch.cpu())\n",
    "    new_batch = batch.transpose(1,0,2,3,4)\n",
    "    return torch.tensor(np.array(new_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd22e312-c77d-4c76-89cf-e571a8b28e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train(model, data):\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        #print(\"\\r\"+str(i), end=\"\")\n",
    "        x = local_transform(x)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        for x_j in x:\n",
    "            y_hat = model(x_j)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "    return loss.item()\n",
    "\n",
    "def test(model, data, label=\"\"):\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y) in enumerate(data):\n",
    "            #x = local_transform(x)\n",
    "            x = x.flatten(start_dim=1).to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            _, y_hat = torch.max(y_hat,1)\n",
    "            correct += (y_hat==y).sum()\n",
    "    print(label+\"accuracy:\", round(float(correct/(i+1)/batch_size), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09def3b9-9e49-45cf-8094-ac78921d4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train\n",
    "N=1\n",
    "for epoch in range(N):\n",
    "    loss = train(model, train_loader)\n",
    "    print(\"  Epoch:\",epoch, \" Loss:\", round(loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbdd3a-93a6-4a3b-a343-fdfc4416bd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf4d36-4338-4911-a08e-c5f08df798e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IN THE NAME OF  ALLAH , THE MOST GRACIOUS, THE MOST MERCIFUL. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25326091-165a-4f57-8abf-7070d8bb0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "modules_path = '../external_modules/'\n",
    "dataset_path = '../../Dataset/STMM'\n",
    "save_path = \"../model\"\n",
    "\n",
    "sys.path.insert(1, modules_path)\n",
    "from video_dataset import VideoFrameDataset, ImglistToTensor\n",
    "from global_defines import GESTURE_MAP_NUM2STR\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f832a2f-7ddf-471d-b04b-3b7233438c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "#img_w = 640\n",
    "#img_h = 480\n",
    "#frames_per_video = 7\n",
    "batch_size = 8\n",
    "num_workers = 1\n",
    "\n",
    "dataset = VideoFrameDataset(\n",
    "    root_path= f\"{dataset_path}\",\n",
    "    annotationfile_path=f\"{dataset_path}/annotations.txt\",\n",
    "    num_segments=8,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='{:01d}.jpg',\n",
    "    transform=ImglistToTensor(),\n",
    "    test_mode=False\n",
    ")\n",
    "class_map = {0:\"down\", 1:\"left\", 2:\"right\", 3:\"up\"}\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [0.7,0.2,0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f642bad4-d373-4a2f-945b-4bd9ce5b6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessings\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.35,\n",
    "    min_tracking_confidence=0.35\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def get_hand_landmarks_frame(frame, transform=False):\n",
    "    if transform==True:\n",
    "        frame = 255*frame\n",
    "        frame = frame.transpose(2,0).numpy().astype('uint8')\n",
    "    results = hands.process(frame)\n",
    "    \n",
    "    hand_landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            hand_landmarks.append([hand_landmark.x, hand_landmark.y, hand_landmark.z])\n",
    "        hand_landmarks = np.array(hand_landmarks)[0]\n",
    "    else:\n",
    "        hand_landmarks = np.zeros(3)\n",
    "\n",
    "    return torch.from_numpy(hand_landmarks.astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33486d79-c9b6-4082-af45-4e3fd03bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "GESTURE_MAP = {'down': 0, 'left': 1, 'right': 2, 'up': 3}\n",
    "def model(batch):\n",
    "    y = torch.zeros(len(batch))\n",
    "    for i, video in enumerate(batch):\n",
    "        prev_s = None\n",
    "        avg_ds = torch.zeros(8, 3)\n",
    "        for s in video:\n",
    "            if prev_s != None:\n",
    "                ds = s - prev_s\n",
    "                avg_ds += ds\n",
    "            prev_s = s\n",
    "        avg_ds = avg_ds / len(video)\n",
    "        dx, dy, dz = avg_ds.sum((0))\n",
    "        # print(dx, dy, dz)\n",
    "\n",
    "        if abs(dx) > abs(dy):\n",
    "            if dx > 0:   y[i] = GESTURE_MAP[\"right\"]\n",
    "            elif dx < 0: y[i] = GESTURE_MAP[\"left\"]\n",
    "        if abs(dx) < abs(dy):\n",
    "            if dy > 0:   y[i] = GESTURE_MAP[\"up\"]\n",
    "            elif dy < 0: y[i] = GESTURE_MAP[\"down\"]\n",
    "        if abs(dx)<1e-1 and (dy)<1e-1:\n",
    "            y[i]=5\n",
    "        print(dx,dy)\n",
    "    return torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2459db1b-6d08-43d1-b6f3-241ff56cd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def test(model, data, label=\"\"):\n",
    "    correct = 0\n",
    "    for k, (x,y) in enumerate(data):\n",
    "        hand_landmarks = torch.zeros(batch_size, 8, 3)\n",
    "\n",
    "        for i, video in enumerate(x):    # x is batch\n",
    "            for j, frame in enumerate(video):\n",
    "                hand_landmarks[i][j] = get_hand_landmarks_frame(frame, True)\n",
    "        \n",
    "        hand_landmarks = hand_landmarks\n",
    "        y_hat = model(hand_landmarks)\n",
    "        correct += (y_hat==y).sum()\n",
    "    print(label+\"accuracy:\", round(float(correct/(k+1)/batch_size), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914755d-5daa-49ad-9039-71fb5db272ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0563009-2392-4ddb-a487-63def436e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, label=\"\"):\n",
    "    hand_landmarks = torch.zeros(1, 8, 3)\n",
    "    \n",
    "    for i, video in enumerate(data):    # x is batch\n",
    "        for j, frame in enumerate(video):\n",
    "            hand_landmarks[i][j] = get_hand_landmarks_frame(frame, False)\n",
    "    \n",
    "    y_hat = model(hand_landmarks)\n",
    "    print(GESTURE_MAP_NUM2STR[y_hat[0].item()])\n",
    "\n",
    "def LiveVideoTest():\n",
    "    # Captures videos\n",
    "    num_batches = 20\n",
    "    batch_size = 1\n",
    "    frames_per_video = 8\n",
    "    frame_rate = 100\n",
    "\n",
    "    for n in range(num_batches):\n",
    "        batch = []\n",
    "        for v in range(batch_size):\n",
    "            video = []\n",
    "            for f in range(frames_per_video):\n",
    "                ret, frame = cam.read()\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                video.append(frame)\n",
    "                time.sleep(1/frame_rate)\n",
    "                \n",
    "            batch.append(video)\n",
    "        predict(model,np.array(batch))\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "try:\n",
    "    LiveVideoTest()\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd7d4ea2-4b3d-43ba-9851-b8aa4a3e6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\AppData\\Local\\Temp\\ipykernel_9484\\2796939799.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(y)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "5.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m cam \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m LiveVideoTest()\n",
      "Cell \u001b[1;32mIn[29], line 29\u001b[0m, in \u001b[0;36mLiveVideoTest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mframe_rate)\n\u001b[0;32m     28\u001b[0m         batch\u001b[38;5;241m.\u001b[39mappend(video)\n\u001b[1;32m---> 29\u001b[0m     predict(model,np\u001b[38;5;241m.\u001b[39marray(batch))\n\u001b[0;32m     30\u001b[0m cam\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     31\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, data, label)\u001b[0m\n\u001b[0;32m      6\u001b[0m         hand_landmarks[i][j] \u001b[38;5;241m=\u001b[39m get_hand_landmarks_frame(frame, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model(hand_landmarks)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(GESTURE_MAP_NUM2STR[y_hat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()])\n",
      "\u001b[1;31mKeyError\u001b[0m: 5.0"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "LiveVideoTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbed8c66-ccd1-49a8-aaeb-f26754102100",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce37ec-f872-4f94-810d-627abfaf4958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
